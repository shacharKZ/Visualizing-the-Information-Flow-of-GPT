# Visualizing the Information Flow of GPT

This repository provides an implementation of the flow-graph modeling of our paper: Interpreting Transformer's Attention Dynamic Memory and Visualizing the Semantic Information Flow of GPT

link: https://arxiv.org/abs/2305.13417

Please try our demo: [![Colab ROME Demo](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1lsdlesXaEsVYwvcJWJac6jcxSr69gKcM?usp=sharing) 

We peovide examples of the modeling plots in the folder "dynamic example". Those are HTML files, please download them and open them in your browser.

Our implementation works with GPT-2 and GPT-J, providing the latter as a guided notebook for adjusting it to other models.

Feel free to open an issue if you find any problems or contact us to discuss any related topics.


## Citation
```bibtex
@misc{katz2023interpreting,
      title={Interpreting Transformer's Attention Dynamic Memory and Visualizing the Semantic Information Flow of GPT}, 
      author={Shahar Katz and Yonatan Belinkov},
      year={2023},
      eprint={2305.13417},
      archivePrefix={arXiv},
}
```
