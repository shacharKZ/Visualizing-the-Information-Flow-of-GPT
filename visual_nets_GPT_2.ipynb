{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly express imports\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib as mpl\n",
    "import plotly.io as pio\n",
    "\n",
    "try:\n",
    "    if 'google.colab' in str(get_ipython()):\n",
    "        print('running on colab. plot will be presented in notebook')\n",
    "    else:\n",
    "        # change to \"browser\" if you want to see the plots in your browser, else omit this line\n",
    "        pio.renderers.default = \"browser\"\n",
    "except:\n",
    "    print('Warning: pio.renderers.default = \"browser\" failed. going to use default renderer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another code we wrote\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1\n",
    "model_name = 'gpt2' # gpt2-small\n",
    "line = 'When Mary and John went to the store, John gave a drink to'\n",
    "target_token = ' Mary' # notice the token includes the space before it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example 2\n",
    "# model_name = 'gpt2-medium'\n",
    "# line = 'The capital of Japan is the city of'\n",
    "# target_token = ' Tokyo' # notice the token includes the space before it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example 3 (from Counterfact)\n",
    "# model_name = 'gpt2-medium'\n",
    "# line = 'Michel Denisot spoke the language of'\n",
    "# target_token = ' French'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # delete the model to free up memory (if more than one model is used)\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.requires_grad_(False)\n",
    "model_config = model.config\n",
    "\n",
    "# collect the hidden states before and after each of those layers (modules)\n",
    "hs_collector = utils.wrap_model(model, layers_to_check = [\n",
    "    '.mlp', '.mlp.c_fc', '.mlp.c_proj',  \n",
    "    '.attn', '.attn.c_attn', '.attn.c_proj', \n",
    "    '.ln_1', '.ln_2', '',])  # '' stands for wrapping transformer.h.<layer_index> in gpt2\n",
    "\n",
    "# add extra functions to the model (like logit lens adjust to the model decoding matrix)\n",
    "model_aux = utils.model_extra(model=model, device='cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "try:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"  # not blocking, just to prevent warnings and faster tokenization\n",
    "except:\n",
    "    pass\n",
    "encoded_line = tokenizer.encode(line.rstrip(), return_tensors='pt').to(device)\n",
    "output_and_cache = model(encoded_line, output_hidden_states=True, output_attentions=True, use_cache=True)\n",
    "\n",
    "hs_collector['past_key_values'] = output_and_cache.past_key_values  # the \"attentnion memory\"\n",
    "hs_collector['attentions'] = output_and_cache.attentions\n",
    "\n",
    "# the final answer token is:\n",
    "model_answer = tokenizer.decode(output_and_cache.logits[0, -1, :].argmax().item())\n",
    "print(f'model_answer: \"{model_answer}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# const definitions\n",
    "# all the \"factor\" values are used to scale the weights of the links between the neurons in the different layers\n",
    "# you are more than welcome to change these values for your own experiments\n",
    "round_digits = 3\n",
    "\n",
    "# the numbers of top and bottom neurons to show at the mlp matricies (mlp.c_fc, mpl.c_proj) and the attention projection matrix (W_O, attn.c_proj)\n",
    "number_of_top_neurons = 20  \n",
    "number_of_bottom_neurons = 10\n",
    "defualt_weight = 7\n",
    "factor_weight_mlp_key_value_link = 1.5\n",
    "n_values_per_head = 2  # number of ki, vi to show for each head. great to examine with gpt2-small/medium but for gpt2-large/xl you might want to reduce this number\n",
    "factor_attn_score = 10\n",
    "factor_head_output = 1.8\n",
    "factor_head_norm = 0.2\n",
    "factor_for_memory_to_head_values = 1.3\n",
    "\n",
    "# if to merge those types of nodes into one node to save space, since they are having only input and output ranks of 1 and used as direct mapping between each other\n",
    "compact_mlp_nodes = False\n",
    "compact_attn_k_v_nodes = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color definitions\n",
    "cmap_node_rank_target = mpl.colors.LinearSegmentedColormap.from_list('cmap_node_rank_target', ['lime', 'greenyellow', 'yellow' , 'yellow', 'yellow'] + ['dimgrey']*53 + ['orangered']*3 + ['red']*4 + ['darkred']*5)\n",
    "cmap_attn_score = mpl.colors.LinearSegmentedColormap.from_list('cmap_attn_score', ['khaki', 'yellow', 'green'])\n",
    "cmap_entropy = mpl.colors.LinearSegmentedColormap.from_list('cmap_entropy', ['darkslategrey', 'lightgrey', 'lightgrey'])\n",
    "\n",
    "backgroud_color = 'black'\n",
    "invisible_link_color = backgroud_color\n",
    "color_for_abstract = 'white'\n",
    "default_color = 'white'\n",
    "link_with_normalizer = 'darkviolet'\n",
    "color_for_bias_vector = 'pink'\n",
    "positive_activation = 'blue'\n",
    "negative_activation = 'red'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_layer(x, debug_msg=None, round_digits=round_digits):\n",
    "    if type(x) == torch.Tensor:\n",
    "        res = torch.norm(x).item()\n",
    "    else:\n",
    "        res = torch.norm(torch.Tensor(x)).item()\n",
    "    if debug_msg:\n",
    "        print(f'{debug_msg}: len(x): {len(x)}, norm: {res}')\n",
    "    if round_digits > 0:\n",
    "        return round(res, round_digits)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_two_nodes(graph_data, index1, index2, prefix1='', prefix2=''):\n",
    "    sources = graph_data['sources']\n",
    "    targets = graph_data['targets']\n",
    "    weights = graph_data['weights']\n",
    "    colors_nodes = graph_data['colors_nodes']\n",
    "    colors_links = graph_data['colors_links']\n",
    "    labels = graph_data['labels']\n",
    "    line_explained = graph_data['line_explained']\n",
    "    customdata = graph_data['customdata']\n",
    "\n",
    "    if index1 +1 != index2 or index2 + 1 != len(labels):\n",
    "        raise Exception(f'The call for merge_two_nodes is not valid. it should be done only if the last two nodes are the ones to merge, \\\n",
    "                         and no other nodes were added after them. got index1: {index1}, index2: {index2}, len(labels): {len(labels)}')\n",
    "\n",
    "    print(f'Start merge_two_nodes for index1: {index1}, index2: {index2}')\n",
    "\n",
    "    merged_label = f'{labels[index1]} > {labels[index2]}'\n",
    "    merged_color = colors_nodes[index2]\n",
    "    merged_customdata = f'{prefix1}{customdata[index1]}<br />{prefix2}{customdata[index2]}'\n",
    "\n",
    "    # pop old nodes and create new one\n",
    "    for _ in range(2):\n",
    "        labels.pop()\n",
    "        colors_nodes.pop()\n",
    "        customdata.pop()\n",
    "    \n",
    "    labels.append(merged_label)\n",
    "    colors_nodes.append(merged_color)\n",
    "    customdata.append(merged_customdata)\n",
    "    new_index = len(labels) - 1\n",
    "\n",
    "    # find the common links and remove them\n",
    "    i = 0\n",
    "    while i < len(sources):\n",
    "        if (sources[i] == index1 and targets[i] == index2) or (sources[i] == index2 and targets[i] == index1):\n",
    "            weights.pop(i)\n",
    "            colors_links.pop(i)\n",
    "            sources.pop(i)\n",
    "            targets.pop(i)\n",
    "            line_explained.pop(i)\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    # find all links with one of the old nodes and change them to the new node\n",
    "    for old_index in [index1, index2]:\n",
    "        i = 0\n",
    "        while i < len(sources):  # len of sources and targets are the same\n",
    "            if sources[i] == old_index:\n",
    "                sources[i] = new_index\n",
    "            if targets[i] == old_index:\n",
    "                targets[i] = new_index\n",
    "            i += 1\n",
    "    \n",
    "    # print(f'Finish merge_two_nodes for index1: {index1}, index2: {index2}')\n",
    "\n",
    "    return new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph_aux(graph_data, title=f'Flow-Graph', save_html=False):\n",
    "    '''\n",
    "    A wrapper for graph plotting by plotly express\n",
    "\n",
    "    @ graph_data: the graph data. see the function @ gen_basic_graph for more details\n",
    "    @ title: the title of the graph\n",
    "    @ save_html: if True, the graph will be saved as an html file to {title}.html. if @ save_html is a non empty string, the graph will be saved as an html file to {save_html}.html\n",
    "    '''\n",
    "    \n",
    "    sources = graph_data['sources']\n",
    "    targets = graph_data['targets']\n",
    "    weights = graph_data['weights']\n",
    "    colors_nodes = graph_data['colors_nodes']\n",
    "    colors_links = graph_data['colors_links']\n",
    "    labels = graph_data['labels']\n",
    "    line_explained = graph_data['line_explained']\n",
    "    customdata = graph_data['customdata']\n",
    "\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "      valueformat = \".0f\",\n",
    "      valuesuffix = \"TWh\",\n",
    "      node = dict(\n",
    "        pad = 15,\n",
    "        thickness = 15,\n",
    "        line = dict(color = backgroud_color, width = 0.5),\n",
    "        label = labels,\n",
    "        color = colors_nodes,\n",
    "        customdata = customdata,\n",
    "        hovertemplate='Node: %{customdata}. %{value}<extra></extra>',\n",
    "      ),\n",
    "      link = dict(\n",
    "        source =  sources,\n",
    "        target =  targets,\n",
    "        value =  weights,\n",
    "        color = colors_links,\n",
    "        customdata = line_explained,\n",
    "        hovertemplate='%{source.customdata}<br />' + ' '*50 + '----[%{customdata},  %{value}]----><br />%{target.customdata}<extra></extra>',\n",
    "      ))]\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        hovermode = 'x',\n",
    "        hoverlabel=dict(font_size=16),\n",
    "        title_text = title,\n",
    "        font=dict(size=12, color='white'),\n",
    "        plot_bgcolor=backgroud_color,\n",
    "        paper_bgcolor=backgroud_color\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # save to html\n",
    "    if save_html != False:\n",
    "      path_out = f'{title}.html' if (type(save_html) != str or save_html == '') else f'{save_html}.html'\n",
    "      fig.write_html(path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_f = copy.deepcopy(utils.rgetattr(model, 'transformer.ln_f')).to(device).requires_grad_(False)\n",
    "decoding_matrix = copy.deepcopy(utils.rgetattr(model, 'lm_head')).to(device).requires_grad_(False)\n",
    "hidden_d = model.config.n_embd\n",
    "\n",
    "def logit_status(hs, wanted_idx):\n",
    "        if wanted_idx == '':\n",
    "                return -1, -1\n",
    "        if type(wanted_idx) == str:\n",
    "                wanted_idx = tokenizer.encode(wanted_idx, add_special_tokens=False)[0]\n",
    "        if type(hs) == torch.Tensor:\n",
    "                hs_tensor = hs.clone().detach().to(device)\n",
    "        else:\n",
    "                hs_tensor = torch.tensor(hs).to(device)\n",
    "        \n",
    "        # logit len including layer normaization with the model final layer norm (ln final)\n",
    "        specific_logics = ln_f(hs_tensor)\n",
    "        # hs to vocab scores\n",
    "        specific_logic_lens = decoding_matrix(specific_logics)\n",
    "        # scores to probabilities\n",
    "        specific_logics = torch.nn.functional.softmax(specific_logic_lens, dim=0)\n",
    "\n",
    "        # get probability of the wanted_idx\n",
    "        prob = specific_logics[wanted_idx].item()\n",
    "        # get ranking of the wanted_idx\n",
    "        smaller = torch.where(specific_logic_lens[wanted_idx] < specific_logic_lens)[0].size()[0]\n",
    "        ranking = smaller+1\n",
    "        # rank 1 -> most probable, rank #vocab_size -> least probable\n",
    "        return prob, ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(probabilities):\n",
    "    # calculates the entropy of a probability distribution\n",
    "    if len(probabilities) != model_config.vocab_size:\n",
    "        probabilities = model_aux.hs_to_probs(probabilities)\n",
    "    # convert the probabilities to a numpy array\n",
    "    probabilities = np.array(probabilities.detach())\n",
    "    # filter out 0 probabilities (to avoid issues with log(0))\n",
    "    non_zero_probs = probabilities[probabilities != 0]\n",
    "    entropy = -np.sum(non_zero_probs * np.log2(non_zero_probs))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_according_to_entropy(hs, max_val=30):\n",
    "    entropy_score = entropy(hs)\n",
    "    color_idx = entropy_score / max_val\n",
    "    color_idx = max(min(color_idx, 1), 0)\n",
    "    color = f'rgba{cmap_entropy(color_idx)}'\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_customdata(hs, prefix='', top_or_bottom='top_k', layer_idx=None, target_word=None, color_flag=True):\n",
    "    '''\n",
    "    return metadata that uses in the graph plot\n",
    "    text: the text that will be displayed in the node when hovering over it\n",
    "    color: the color of the node accodring to its probability of the target word \n",
    "    (green if very probable, red if very improbable and grey otherwise)\n",
    "    '''\n",
    "\n",
    "    color = default_color \n",
    "    hs_meaning = model_aux.hs_to_token_top_k(hs, k_top=5, k_bottom=5)\n",
    "    res = f'{hs_meaning[top_or_bottom]}'\n",
    "\n",
    "    if prefix != '':\n",
    "        res = f'{prefix}: {res}'\n",
    "    if layer_idx is not None:\n",
    "        res = f'{layer_idx}) {res}'\n",
    "    if target_word is not None:\n",
    "        prob, ranking = logit_status(hs, target_word)\n",
    "        prob = round(prob*100, round_digits)  # probs [0,1] as percentage [0,100]\n",
    "        res = f'{res} [status: \"{target_word}\": prob:{prob}%, rank: {ranking})]'\n",
    "        if color_flag:\n",
    "            color_idx = ranking/model_config.vocab_size  # color index [0,1] (according to ranking)\n",
    "            color = f'rgba{cmap_node_rank_target(color_idx)}'\n",
    "    return res, color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_mlp_to_graph(layer_idx: int, graph_data, model, hs_collector, row_idx=-1, model_aux=model_aux, target_word=None):\n",
    "    '''\n",
    "    create a subgraph of the feed-forward (FF, MLP) part of the model at layer_idx\n",
    "    the subgraph is a graph of the neurons in the FF part of the model\n",
    "    the nodes are the most active neurons in the FF (some of positive and some of negative)\n",
    "    the links are the connections between the neurons (summation of the neurons or when one neuron creates the coefficient of another neuron)\n",
    "    the graph is created using the graph_data dictionary\n",
    "    if the graph_data dictionary's lists are empty, they will be initialized\n",
    "    if the graph_data dictionary 's lists are not empty, they will be updated (try to connect the new nodes to the existing nodes)\n",
    "\n",
    "    @ layer_idx: the index of the layer in the model\n",
    "    @ graph_data: the graph data dictionary (if called first time, should include empty list for the keys it uses)\n",
    "    @ model: the model (for example: gpt2)\n",
    "    @ hs_collector: the hs_collector dictionary (created from wrapping the model with the hs_collector class)\n",
    "    @ row_idx: the index of the row in the hs_collector which correspond to the infrence of the #row_idx token. use -1 for the last token (Note: currently not supported any other value than -1)\n",
    "    @ model_aux: the model_aux class (more functions for the original model)\n",
    "    @ target_word: the target word for extracting the status of the neurons (ranking and probability)    \n",
    "    '''\n",
    "    sources = graph_data['sources']\n",
    "    targets = graph_data['targets']\n",
    "    weights = graph_data['weights']\n",
    "    colors_nodes = graph_data['colors_nodes']\n",
    "    colors_links = graph_data['colors_links']\n",
    "    labels = graph_data['labels']\n",
    "    line_explained = graph_data['line_explained']\n",
    "    customdata = graph_data['customdata']\n",
    "\n",
    "    mlp_residual = hs_collector[layer_idx]['ln_2']['input'][row_idx]  # mlp_residual == attn_part_out == attn_output+attn_residual\n",
    "    mlp_input = hs_collector[layer_idx]['mlp']['input'][row_idx]  # mlp_input == c_fc_input\n",
    "    mlp_out = hs_collector[layer_idx]['mlp']['output'][row_idx]  # only the output of the mlp (not with the residual)\n",
    "    block_out =hs_collector[layer_idx]['']['output'][row_idx]  # mlp_residual + mlp_out\n",
    "\n",
    "    mlp_residual_meaning = model_aux.hs_to_token_top_k(mlp_residual, k_top=1, k_bottom=0)\n",
    "    mlp_input_meaning = model_aux.hs_to_token_top_k(mlp_input, k_top=1, k_bottom=0)\n",
    "    mlp_out_meaning = model_aux.hs_to_token_top_k(mlp_out, k_top=1, k_bottom=0)\n",
    "    block_out_meaning = model_aux.hs_to_token_top_k(block_out, k_top=1, k_bottom=0)\n",
    "\n",
    "    # try to connect to the previous sub blocks (attn_part_out if exists)\n",
    "    if 'idx_attn_part_out' in graph_data:\n",
    "         # uses the previous attn_part_out node in the graph as the input to this block, which is actually the residual of this block\n",
    "         idx_mlp_residual = graph_data['idx_attn_part_out']  # should contain a int value with the index of the previous attn_part_out node in the graph\n",
    "    else:\n",
    "        # there is no problem of not having the previous attn_part_out node in the graph. it will assume this is the start of the graph\n",
    "        print(f'WARNING: did not find idx_attn_part_out for layer_idx={layer_idx}')\n",
    "        # if no previous node we create a node to represent the input to the block (which is actually the resiual of this block)\n",
    "        labels.append(mlp_residual_meaning['top_k'][0])\n",
    "        curr_metadata, curr_color = get_node_customdata(mlp_residual, prefix='mlp_residual', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "        colors_nodes.append(curr_color)\n",
    "        customdata.append(curr_metadata)\n",
    "        idx_mlp_residual = len(labels) - 1\n",
    "\n",
    "    # create nodes for the input after layer norm (mlp_input), output (mlp alone, mlp_out), and the output of the mlp with the residual (residual+mlp_out)\n",
    "    labels.append(mlp_input_meaning['top_k'][0])\n",
    "    curr_metadata, curr_color = get_node_customdata(mlp_input, prefix='mlp_input', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(curr_color)\n",
    "    idx_mlp_input = len(labels) - 1\n",
    "\n",
    "    labels.append(mlp_out_meaning['top_k'][0])\n",
    "    curr_metadata, curr_color = get_node_customdata(mlp_out, prefix='mlp_out', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(curr_color)\n",
    "    idx_mlp_out = len(labels) - 1\n",
    "\n",
    "    labels.append(block_out_meaning['top_k'][0])\n",
    "    curr_metadata, curr_color = get_node_customdata(block_out, prefix='residual+mlp_out', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(curr_color)\n",
    "    idx_block_out = len(labels) - 1\n",
    "\n",
    "    # connect the last 4 nodes (input, input after layer norm, ouptut of mlp, output of mlp with residual)\n",
    "    sources.append(idx_mlp_residual)\n",
    "    targets.append(idx_mlp_input)\n",
    "    curr_norm = get_norm_layer(mlp_residual)\n",
    "    weights.append(get_norm_layer(mlp_residual))\n",
    "    colors_links.append(link_with_normalizer)\n",
    "    line_explained.append(f'norm:{curr_norm}')\n",
    "\n",
    "    sources.append(idx_mlp_residual)\n",
    "    targets.append(idx_block_out)\n",
    "    curr_norm = get_norm_layer(mlp_residual)\n",
    "    weights.append(curr_norm)\n",
    "    colors_links.append('rgba(51,102,153,0.3)')  # a unique color for the mlp residual\n",
    "    line_explained.append(f'norm:{curr_norm}')\n",
    "\n",
    "    sources.append(idx_mlp_out)\n",
    "    targets.append(idx_block_out)\n",
    "    curr_norm = get_norm_layer(mlp_out)\n",
    "    weights.append(curr_norm)\n",
    "    colors_links.append(get_color_according_to_entropy(mlp_out))\n",
    "    line_explained.append(f'norm:{curr_norm}')\n",
    "\n",
    "    # get the first and second matricies of the mlp\n",
    "    c_fc = utils.rgetattr(model, f\"transformer.h.{layer_idx}.mlp.c_fc.weight\").clone().detach().cpu()\n",
    "    c_proj = utils.rgetattr(model, f\"transformer.h.{layer_idx}.mlp.c_proj.weight\").clone().detach().cpu()\n",
    "\n",
    "    values_norm = c_proj.norm(dim=1)\n",
    "    hs =  hs_collector[layer_idx]['mlp.c_proj']['input'][row_idx]  # value activation. mid results between the key and value matrix\n",
    "    hs_mul_norm = hs * values_norm  # this is our metric for the importance of the value activation (value*norm of the second matrix)\n",
    "    \n",
    "    # pick the top most activate neurons (according to activasion sign)\n",
    "    for case, n_top, is_largest in [('top_k', number_of_top_neurons, True), ('bottom_k', number_of_bottom_neurons, False)]:\n",
    "        tops = torch.topk(hs_mul_norm, k=n_top, largest=is_largest)\n",
    "        for entry_idx, activision_value_mul_norm in zip(tops.indices, tops.values):\n",
    "            activision_value_mul_norm = round(activision_value_mul_norm.item(), round_digits)\n",
    "            activision_value = round(hs[entry_idx].item(), round_digits)\n",
    "            entry_idx = entry_idx.item()\n",
    "\n",
    "            # create a node for the \"key\" neuron (the first matrix)\n",
    "            idx_key = len(labels)\n",
    "            idx_value = idx_key+1\n",
    "            curr_c_fc_meaning = model_aux.hs_to_token_top_k(c_fc.T[entry_idx], k_top=1, k_bottom=0)\n",
    "            labels.append(curr_c_fc_meaning['top_k'][0])\n",
    "            curr_metadata, curr_color = get_node_customdata(c_fc.T[entry_idx], prefix=f'key{entry_idx}', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "            customdata.append(curr_metadata)\n",
    "            colors_nodes.append(curr_color)\n",
    "\n",
    "            # create a node for the \"value\" neuron (the second matrix)\n",
    "            curr_c_proj_meaning = model_aux.hs_to_token_top_k(c_proj[entry_idx], k_top=1, k_bottom=1)\n",
    "            labels.append(curr_c_proj_meaning[case][0])  # value is chosen accodring to activation sign. suppouse to reflect the meaning its adding to the output\n",
    "            curr_metadata, curr_color = get_node_customdata(c_proj[entry_idx], prefix=f'value{entry_idx}', top_or_bottom=case, layer_idx=layer_idx, target_word=target_word)\n",
    "            customdata.append(curr_metadata)\n",
    "            colors_nodes.append(curr_color)\n",
    "\n",
    "            \n",
    "            if compact_mlp_nodes:\n",
    "                merged_idx = merge_two_nodes(graph_data, index1=idx_key, index2=idx_value, prefix1='key:', prefix2='value:')\n",
    "                idx_key = merged_idx\n",
    "                idx_value = merged_idx\n",
    "            else:\n",
    "                # connect the \"key\" and the \"value\"\n",
    "                sources.append(idx_key)\n",
    "                targets.append(idx_value)\n",
    "                weights.append(abs(activision_value)*factor_weight_mlp_key_value_link)\n",
    "                colors_links.append(positive_activation if is_largest else negative_activation)\n",
    "                line_explained.append(f'activision:{activision_value}')\n",
    "\n",
    "            # add between the mlp_input and the \"key\" neuron\n",
    "            sources.append(idx_mlp_input)\n",
    "            targets.append(idx_key)\n",
    "            weights.append(defualt_weight)\n",
    "            colors_links.append(get_color_according_to_entropy(c_fc.T[entry_idx]))\n",
    "            line_explained.append(f'')\n",
    "\n",
    "            # add a link between of the \"value\" neuron to the mlp_out\n",
    "            sources.append(idx_value)\n",
    "            targets.append(idx_mlp_out)\n",
    "            weights.append(abs(activision_value_mul_norm))\n",
    "            colors_links.append(get_color_according_to_entropy(c_proj[entry_idx]))\n",
    "            line_explained.append(f'activision*norm:{abs(activision_value_mul_norm)}')\n",
    "    \n",
    "    # we also add neurons representing the matricies bias vectors\n",
    "    # we create nodes for each matricies bias vector then connect them to the flow\n",
    "    c_fc_bias = utils.rgetattr(model, f\"transformer.h.{layer_idx}.mlp.c_fc.bias\").clone().detach().cpu() @ c_proj\n",
    "    c_fc_bias_meaning = model_aux.hs_to_token_top_k(c_fc_bias, k_top=1, k_bottom=0)\n",
    "    labels.append(c_fc_bias_meaning['top_k'][0])\n",
    "    curr_metadata, curr_color = get_node_customdata(c_fc_bias, prefix='c_fc_bias', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(color_for_bias_vector) # special color for bias\n",
    "    idx_c_fc_bias = len(labels) - 1\n",
    "\n",
    "    c_proj_bias = utils.rgetattr(model, f\"transformer.h.{layer_idx}.mlp.c_proj.bias\").clone().detach().cpu()\n",
    "    c_proj_bias_meaning = model_aux.hs_to_token_top_k(c_proj_bias, k_top=1, k_bottom=0)\n",
    "    labels.append(c_proj_bias_meaning['top_k'][0])\n",
    "    curr_metadata, curr_color = get_node_customdata(c_proj_bias, prefix='c_proj_bias', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(color_for_bias_vector)\n",
    "    idx_c_proj_bias = len(labels) - 1\n",
    "\n",
    "    if compact_mlp_nodes:\n",
    "        merged_idx = merge_two_nodes(graph_data, index1=idx_c_fc_bias, index2=idx_c_proj_bias, prefix1='c_fc_bias:', prefix2='c_proj_bias:')\n",
    "        idx_c_fc_bias = merged_idx\n",
    "        idx_c_proj_bias = merged_idx\n",
    "    else:\n",
    "        # connect c_fc_bias to c_proj_bias, although it is not really a link (this why its color is invisible_link_color)\n",
    "        sources.append(idx_c_fc_bias)\n",
    "        targets.append(idx_c_proj_bias)\n",
    "        weights.append(0.05) # to be barely visible (trick to make it unvisible with the background)\n",
    "        colors_links.append(invisible_link_color)\n",
    "        line_explained.append(f'norm:{curr_norm}')\n",
    "\n",
    "    # connect mlp_input to c_fc_bias\n",
    "    sources.append(idx_mlp_input)\n",
    "    targets.append(idx_c_fc_bias)\n",
    "    curr_norm = get_norm_layer(mlp_input)\n",
    "    weights.append(curr_norm)\n",
    "    colors_links.append(color_for_bias_vector)\n",
    "    line_explained.append(f'norm:{curr_norm}')\n",
    "\n",
    "    # connect c_proj_bias to mlp_out\n",
    "    sources.append(idx_c_proj_bias)\n",
    "    targets.append(idx_mlp_out)\n",
    "    curr_norm = get_norm_layer(c_proj_bias)\n",
    "    weights.append(curr_norm)\n",
    "    colors_links.append(color_for_bias_vector)\n",
    "    line_explained.append(f'norm:{curr_norm}')\n",
    "\n",
    "    # update index to last mlp_out so the next attention block will connect to it\n",
    "    graph_data['idx_block_out'] = idx_block_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_attn_to_graph(layer_idx, graph_data, hs_collector, model, row_idx=-1, model_aux=model_aux, target_word=None, line=None):\n",
    "    '''\n",
    "    create a graph for the attention (attn) layer\n",
    "    the subgraph is a graph of the neurons in the Q, K, V O matricies, mostly aggregated into heads\n",
    "    the nodes are single or small groups of neurons (when they are aggregated into heads or subheads)\n",
    "    the links are the connections between the neurons (summation of the neurons or when one neuron creates the coefficient of another neuron)\n",
    "    the graph is created using the graph_data dictionary\n",
    "    if the graph_data dictionary's lists are empty, they will be initialized\n",
    "    if the graph_data dictionary 's lists are not empty, they will be updated (try to connect the new nodes to the existing nodes)\n",
    "\n",
    "    @ layer_idx: the index of the layer in the model\n",
    "    @ graph_data: the graph data dictionary (if called first time, should include empty list for the keys it uses)\n",
    "    @ model: the model (for example: gpt2)\n",
    "    @ hs_collector: the hs_collector dictionary (created from wrapping the model with the hs_collector class)\n",
    "    @ row_idx: the index of the row in the hs_collector which correspond to the infrence of the #row_idx token. use -1 for the last token (Note: currently not supported any other value than -1)\n",
    "    @ model_aux: the model_aux class (more functions for the original model)\n",
    "    @ target_word: the target word for extracting the status of the neurons (ranking and probability)\n",
    "    @ line: the line that was used to generate the data in hs_collector (the prompt to the model)\n",
    "    '''\n",
    "    sources = graph_data['sources']\n",
    "    targets = graph_data['targets']\n",
    "    weights = graph_data['weights']\n",
    "    colors_nodes = graph_data['colors_nodes']\n",
    "    colors_links = graph_data['colors_links']\n",
    "    labels = graph_data['labels']\n",
    "    line_explained = graph_data['line_explained']\n",
    "    customdata = graph_data['customdata']\n",
    "\n",
    "    # uses to show what was the token that generated the attention memory (previous keys and layer)\n",
    "    # the i-th key and i-th value were generated by the i-th token\n",
    "    # if @line is not given (None) - will not show this information\n",
    "    parsed_line = None\n",
    "    if line is not None:\n",
    "        parsed_line = tokenizer.encode(line, return_tensors='pt')\n",
    "        # save the parsed line for later use\n",
    "        parsed_line = [tokenizer.decode(x.item()) for x in parsed_line[0]]\n",
    "\n",
    "    \n",
    "    attn_residual = hs_collector[layer_idx]['']['input'][row_idx]\n",
    "    attn_residual_meaning = model_aux.hs_to_token_top_k(attn_residual, k_top=1, k_bottom=0)\n",
    "\n",
    "    # try to connect to previous layer\n",
    "    if 'idx_block_out' in graph_data: # previous layer exists\n",
    "        idx_attn_residual = graph_data['idx_block_out']\n",
    "    else:\n",
    "        # there is no problem of not having the previous idx_block_out node in the graph. it will assume this is the start of the graph\n",
    "        print(f'WARNING: did not find idx_block_out for layer_idx={layer_idx}')\n",
    "        # create the input (before layer norm) to the attn block, which is actually the residual from the previous block\n",
    "        labels.append(attn_residual_meaning['top_k'][0])\n",
    "        curr_metadata, curr_color = get_node_customdata(attn_residual, prefix=f'attn_residual', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "        customdata.append(curr_metadata)\n",
    "        colors_nodes.append(curr_color)\n",
    "        idx_attn_residual = len(labels) - 1\n",
    "\n",
    "    # create nodes for the attention input (after layer norm) and for the attention output\n",
    "    # attn input (after layer norm. the layer norm is done before the attentnion module)\n",
    "    attn_input = hs_collector[layer_idx]['attn']['input'][row_idx]\n",
    "    attn_input_meaning = model_aux.hs_to_token_top_k(attn_input, k_top=1, k_bottom=0)\n",
    "    labels.append(attn_input_meaning['top_k'][0])\n",
    "    curr_metadata, curr_color = get_node_customdata(attn_input, prefix=f'attn_input', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(curr_color)\n",
    "    idx_attn_input = len(labels) - 1\n",
    "\n",
    "    # attn output\n",
    "    attn_out = hs_collector[layer_idx]['attn']['output'][row_idx]\n",
    "    attn_out_meaning = model_aux.hs_to_token_top_k(attn_out, k_top=1, k_bottom=0)\n",
    "    labels.append(attn_out_meaning['top_k'][0])\n",
    "    curr_metadata, curr_color = get_node_customdata(attn_out, prefix=f'attn_out', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(curr_color)\n",
    "    idx_attn_out = len(labels) - 1\n",
    "\n",
    "    # link between prev_block_output/attn_residual and attn_input (the process of ln_1)\n",
    "    sources.append(idx_attn_residual)\n",
    "    targets.append(idx_attn_input)\n",
    "    curr_weight = get_norm_layer(attn_residual)\n",
    "    weights.append(curr_weight)\n",
    "    colors_links.append(link_with_normalizer)\n",
    "    line_explained.append(f'norm:{curr_weight}')\n",
    "\n",
    "    c_attn = utils.rgetattr(model, f\"transformer.h.{layer_idx}.attn.c_attn.weight\").clone().detach().cpu() # W_QKV (the QKV matrix)\n",
    "    c_proj = utils.rgetattr(model, f\"transformer.h.{layer_idx}.attn.c_proj.weight\").clone().detach().cpu() # W_O (the Output/projection matrix)\n",
    "\n",
    "    # in gpt2 c_attn is the concatenation of Wq, Wk, Wv (Q, K, V)\n",
    "    Wq = c_attn[:, :hidden_d]\n",
    "    Wk = c_attn[:, hidden_d:2*hidden_d]\n",
    "    Wv = c_attn[:, 2*hidden_d:]\n",
    "\n",
    "    # we can get the output of each of the Q, K, V matrices by splitting the output of the c_attn\n",
    "    c_attn_output = hs_collector[layer_idx]['attn.c_attn']['output'][row_idx].cpu()\n",
    "    q = c_attn_output[ :hidden_d]  # this layer query\n",
    "    k = c_attn_output[hidden_d:2*hidden_d]  # this layer key. it is added to the attention memory (to \"past_key_values\" so also the next tokens can use it)\n",
    "    v = c_attn_output[2*hidden_d:] # this layer value. like the key, it is added to the attention memory \n",
    "\n",
    "    # projection using the QK circuit\n",
    "    def pre_project_q(hs_q):\n",
    "        return hs_q @ Wk\n",
    "\n",
    "    # projection using the QK circuit\n",
    "    def pre_project_k(hs_k):\n",
    "        return Wq @ hs_k\n",
    "    \n",
    "    # projection using the OV circuit\n",
    "    def pre_project_v(hs_v):\n",
    "        return hs_v @ c_proj\n",
    "    \n",
    "    q_projected = pre_project_q(q)\n",
    "    k_projected = pre_project_k(k)\n",
    "    v_poject = pre_project_v(v)  \n",
    "\n",
    "    # create a node for q,k,v together\n",
    "    q_meaning = model_aux.hs_to_token_top_k(q_projected, k_top=1, k_bottom=0)\n",
    "    q_data, curr_color = get_node_customdata(q_projected, prefix=f'q (for current calc)', top_or_bottom='top_k', target_word=target_word)\n",
    "    k_data, _ = get_node_customdata(k_projected, prefix=f'k (for next tokens)', top_or_bottom='top_k', target_word=target_word)\n",
    "    v_data, _ = get_node_customdata(v_poject, prefix=f'v (for next tokens)', top_or_bottom='top_k', target_word=target_word)\n",
    "\n",
    "    curr_metadata = f'q,k,v (before splitting into heads):' + '<br />' + q_data + '<br />' + k_data + '<br />' + v_data\n",
    "\n",
    "    # create a new node for query (q) and add the key and value (k, v) metadata. we call this node qkv_full\n",
    "    labels.append(q_meaning['top_k'][0])\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(curr_color)\n",
    "    idx_qkv_full = len(labels) - 1\n",
    "\n",
    "    # connect between attn_input and qkv_full   \n",
    "    sources.append(idx_attn_input)\n",
    "    targets.append(idx_qkv_full)\n",
    "    curr_weight = get_norm_layer(attn_input)\n",
    "    weights.append(curr_weight)\n",
    "    colors_links.append(get_color_according_to_entropy(q))\n",
    "    line_explained.append(f'norm:{curr_weight}')\n",
    "\n",
    "    # the concated results from all the heads but without the OV circuit projection\n",
    "    concated_heads_wihtout_projection = hs_collector[layer_idx]['attn.c_proj']['input'][row_idx]\n",
    "\n",
    "    idx_concated_heads = len(labels)  # attn_c_proj input\n",
    "    concated_heads_without_projection_meaning = model_aux.hs_to_token_top_k(concated_heads_wihtout_projection, k_top=1, k_bottom=0)\n",
    "    labels.append(concated_heads_without_projection_meaning['top_k'][0])\n",
    "    curr_metadata, curr_color = get_node_customdata(concated_heads_wihtout_projection, prefix=f'concated_heads (without projection)', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(color_for_abstract)\n",
    "\n",
    "    # link attn_output_and_residual to block_output\n",
    "    attn_output_and_residual = attn_out + attn_residual # equal to hs_collector[layer_idx]['ln_2']['input'][row_idx]\n",
    "    attn_output_and_residual_meaning = model_aux.hs_to_token_top_k(attn_output_and_residual, k_top=1, k_bottom=0)\n",
    "    labels.append(attn_output_and_residual_meaning['top_k'][0])\n",
    "    curr_metadata, curr_color = get_node_customdata(attn_output_and_residual, prefix=f'attn_out+residual', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(curr_color)\n",
    "    idx_attn_output_and_residual = len(labels) - 1\n",
    "\n",
    "    # link attn_output_and_residual to attn_c_proj (the summation of this attn block to the residual)\n",
    "    sources.append(idx_attn_out)\n",
    "    targets.append(idx_attn_output_and_residual)\n",
    "    curr_weight = get_norm_layer(attn_out)\n",
    "    weights.append(curr_weight)\n",
    "    colors_links.append(get_color_according_to_entropy(attn_out))\n",
    "    line_explained.append(f'norm: {curr_weight}')\n",
    "\n",
    "    # the residual connection\n",
    "    sources.append(idx_attn_residual)\n",
    "    targets.append(idx_attn_output_and_residual)\n",
    "    curr_weight = get_norm_layer(attn_residual)\n",
    "    weights.append(curr_weight)\n",
    "    colors_links.append('rgba(102,0,204,0.3)')  # unique color for the attn residual\n",
    "    line_explained.append(f'norm: {curr_weight}')\n",
    "\n",
    "    # print the hidden meaning of each of the heads\n",
    "    dim_head = model.config.n_embd // model.config.n_head\n",
    "\n",
    "    # create for each head the following nodes:\n",
    "    # (1) the qi - this head part in the query q (we also add its information about ki, vi that were generated at this layer and saved to the attention memory)\n",
    "    # (2) its #n_values_per_head top biggest ki and vi (the keys and values from the attention memory) accodring to the attention score\n",
    "    # (3) the head output - the weighted summation of all the vi into it \n",
    "    for head_idx in range(model.config.n_head):\n",
    "        # hs_collector['past_key_values'][layer_idx][0 for key, 1 for value][entry in batch][head_idx] -> list of the keys/values for this head. the i-th entry is the key/value for the i-th token in the input\n",
    "        keys = hs_collector['past_key_values'][layer_idx][0][0][head_idx]  \n",
    "        values = hs_collector['past_key_values'][layer_idx][1][0][head_idx]\n",
    "        attentions = hs_collector['attentions'][layer_idx][0][head_idx][row_idx]\n",
    "\n",
    "        # qi with the information about ki, vi (1)\n",
    "        qi = q[dim_head * head_idx: dim_head * (head_idx + 1)]\n",
    "        ki = k[dim_head * head_idx: dim_head * (head_idx + 1)]\n",
    "        vi = v[dim_head * head_idx: dim_head * (head_idx + 1)]\n",
    "\n",
    "        q_i_fill = torch.zeros(model.config.n_embd)\n",
    "        q_i_fill[dim_head*head_idx:dim_head*(head_idx+1)] = qi\n",
    "        q_i_projected = pre_project_q(q_i_fill)\n",
    "\n",
    "        k_i_fill = torch.zeros(model.config.n_embd)\n",
    "        k_i_fill[dim_head*head_idx:dim_head*(head_idx+1)] = ki\n",
    "        k_i_projected = pre_project_k(k_i_fill)\n",
    "\n",
    "        v_i_fill = torch.zeros(model.config.n_embd)\n",
    "        v_i_fill[dim_head*head_idx:dim_head*(head_idx+1)] = vi\n",
    "        v_i_projected = pre_project_v(v_i_fill) \n",
    "\n",
    "        q_i_projected_meaning = model_aux.hs_to_token_top_k(q_i_projected, k_top=1, k_bottom=0)\n",
    "        q_data, curr_color = get_node_customdata(q_i_projected, prefix=f'qi (for current calc)', top_or_bottom='top_k', target_word=target_word)\n",
    "        k_data, _ = get_node_customdata(k_i_projected, prefix=f'ki (for next tokens)', top_or_bottom='top_k', target_word=target_word)\n",
    "        v_data, _ = get_node_customdata(v_i_projected, prefix=f'vi (for next tokens)', top_or_bottom='top_k', target_word=target_word)\n",
    "\n",
    "        curr_metadata = f'head {head_idx}:' + '<br />' + q_data + '<br />' + k_data + '<br />' + v_data\n",
    "\n",
    "        # create a new node for query and add the key and value metadata\n",
    "        labels.append(q_i_projected_meaning['top_k'][0])\n",
    "        customdata.append(curr_metadata)\n",
    "        colors_nodes.append(curr_color)\n",
    "        idx_q_i_projected = len(labels) - 1\n",
    "\n",
    "        # create a link between the input and the head query \n",
    "        sources.append(idx_qkv_full)\n",
    "        targets.append(idx_q_i_projected)\n",
    "        curr_weight = get_norm_layer(q_i_projected)\n",
    "        weights.append(curr_weight*factor_head_norm)\n",
    "        colors_links.append(get_color_according_to_entropy(q_i_projected))\n",
    "        line_explained.append(f'norm: {curr_weight}')\n",
    "\n",
    "        # create a new node for head output (3)\n",
    "        head_output = torch.zeros(model.config.n_embd)\n",
    "        head_output[dim_head*head_idx:dim_head*(head_idx+1)] = concated_heads_wihtout_projection[dim_head*head_idx:dim_head*(head_idx+1)]\n",
    "        head_output_projected = pre_project_v(head_output)\n",
    "        head_output_projected_meaning = model_aux.hs_to_token_top_k(head_output_projected, k_top=1, k_bottom=0)\n",
    "\n",
    "        labels.append(head_output_projected_meaning['top_k'][0])\n",
    "        curr_metadata, curr_color = get_node_customdata(head_output_projected, prefix=f'head {head_idx}: output', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "        customdata.append(curr_metadata)\n",
    "        colors_nodes.append(curr_color)\n",
    "        idx_head_output_projected = len(labels) - 1\n",
    "\n",
    "        # create a link between the head output and the concated heads\n",
    "        sources.append(idx_head_output_projected)\n",
    "        targets.append(idx_concated_heads)\n",
    "        curr_weight = get_norm_layer(head_output_projected)\n",
    "        weights.append(curr_weight*factor_head_output)\n",
    "        colors_links.append(get_color_according_to_entropy(head_output_projected))\n",
    "        line_explained.append(f'norm after projection:{curr_weight}')\n",
    "        \n",
    "\n",
    "        # the nodes representing the #n_values_per_head top biggest ki and vi (2)\n",
    "        best_head_vals = attentions.topk(n_values_per_head, dim=0)\n",
    "        for attn_val_idx, attn_score in zip(best_head_vals.indices, best_head_vals.values):\n",
    "            attn_score = round(attn_score.item(), round_digits)\n",
    "            keys_from_attn = keys[attn_val_idx]  # should be in the size of the subhead (for example, 64 for gpt2-medium)\n",
    "            values_from_attn = values[attn_val_idx]  # should be in the size of the subhead\n",
    "\n",
    "            keys_from_attn_proj = torch.zeros(model.config.n_embd)\n",
    "            keys_from_attn_proj[dim_head*head_idx:dim_head*(head_idx+1)] = keys_from_attn\n",
    "            keys_from_attn_proj = pre_project_k(keys_from_attn_proj)\n",
    "\n",
    "            values_from_attn_proj = torch.zeros(model.config.n_embd)\n",
    "            values_from_attn_proj[dim_head*head_idx:dim_head*(head_idx+1)] = values_from_attn\n",
    "            values_from_attn_proj = pre_project_v(values_from_attn_proj)\n",
    "\n",
    "            keys_from_attn_proj_meaning = model_aux.hs_to_token_top_k(keys_from_attn_proj, k_top=1, k_bottom=0)\n",
    "            values_from_attn_proj_meaning = model_aux.hs_to_token_top_k(values_from_attn_proj, k_top=1, k_bottom=0)\n",
    "\n",
    "            # create node for the top keys ki\n",
    "            labels.append(keys_from_attn_proj_meaning['top_k'][0])\n",
    "            if parsed_line is not None:\n",
    "                curr_metadata, curr_color = get_node_customdata(keys_from_attn_proj, prefix=f'head {head_idx}: key {attn_val_idx} [created from \"{parsed_line[attn_val_idx]}\"]', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "            else:\n",
    "                curr_metadata, curr_color = get_node_customdata(keys_from_attn_proj, prefix=f'head {head_idx}: key {attn_val_idx}', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "            customdata.append(curr_metadata)\n",
    "            colors_nodes.append(curr_color)\n",
    "            idx_keys_from_attn_proj = len(labels) - 1\n",
    "\n",
    "            # create node for the top values vi\n",
    "            labels.append(values_from_attn_proj_meaning['top_k'][0])\n",
    "            if parsed_line is not None:\n",
    "                curr_metadata, curr_color = get_node_customdata(values_from_attn_proj, prefix=f'head {head_idx}: value {attn_val_idx} [created from \"{parsed_line[attn_val_idx]}\"]', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "            else:\n",
    "                curr_metadata, curr_color = get_node_customdata(values_from_attn_proj, prefix=f'head {head_idx}: value {attn_val_idx}', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "            customdata.append(curr_metadata)\n",
    "            colors_nodes.append(curr_color)\n",
    "            idx_values_from_attn_proj = len(labels) - 1\n",
    "\n",
    "            # create a link between the query qi and the top keys ki\n",
    "            color_attn = f'rgba{cmap_attn_score(attn_score)}'\n",
    "\n",
    "            sources.append(idx_q_i_projected)\n",
    "            targets.append(idx_keys_from_attn_proj)\n",
    "            curr_weight = get_norm_layer(q_i_projected)\n",
    "            weights.append(max(attn_score*factor_attn_score, 0.51))  # in case the attentnion score is to low and the line might not be visible\n",
    "            colors_links.append(color_attn)\n",
    "            line_explained.append(f'attention score: {attn_score} (qi norm: {curr_weight})')\n",
    "\n",
    "            if compact_attn_k_v_nodes:\n",
    "                merged_idx = merge_two_nodes(graph_data, index1=idx_keys_from_attn_proj, index2=idx_values_from_attn_proj, prefix1='ki:', prefix2='vi:')\n",
    "                idx_keys_from_attn_proj = merged_idx\n",
    "                idx_values_from_attn_proj = merged_idx\n",
    "            else:\n",
    "                # create the link between the top keys and the top values\n",
    "                sources.append(idx_keys_from_attn_proj)\n",
    "                targets.append(idx_values_from_attn_proj)\n",
    "                curr_weight = get_norm_layer(keys_from_attn_proj)\n",
    "                weights.append(max(attn_score*factor_attn_score, 0.51))\n",
    "                colors_links.append(color_attn)\n",
    "                line_explained.append(f'attention score: {attn_score} (ki norm: {curr_weight})')\n",
    "\n",
    "            # create a link between the top values and the idx_head_output_projected\n",
    "            sources.append(idx_values_from_attn_proj)\n",
    "            targets.append(idx_head_output_projected)\n",
    "            curr_weight = get_norm_layer(values_from_attn_proj)\n",
    "            weights.append(max(attn_score*curr_weight*factor_for_memory_to_head_values, 0.51))\n",
    "            colors_links.append(get_color_according_to_entropy(values_from_attn_proj))\n",
    "            line_explained.append(f'attention score * norm: {attn_score*curr_weight} (vi norm: {curr_weight})')\n",
    "    \n",
    "    # now we want to present single neurons from the concatenated heads and how they are projected (indevideually) to the output by W_O (the output projection matrix)\n",
    "    # we pick only the top most activated neurons (positive and negative)\n",
    "    concated_heads_wihtout_projection_mul_norm = concated_heads_wihtout_projection * c_proj.norm(dim=1)\n",
    "    for case, n_top, is_largest in [('top_k', number_of_top_neurons, True), ('bottom_k', number_of_bottom_neurons, False)]:\n",
    "        tops = torch.topk(concated_heads_wihtout_projection_mul_norm, k=n_top, largest=is_largest)\n",
    "        for entry_idx, activision_mul_norm in zip(tops.indices, tops.values):\n",
    "            activision_mul_norm = round(activision_mul_norm.item(), round_digits)\n",
    "            entry_idx = entry_idx.item()\n",
    "            activision_value = round(concated_heads_wihtout_projection[entry_idx].item(), round_digits)\n",
    "                \n",
    "            # connect between c_proj_input, which is the concatenated heads, and each of this neurons\n",
    "            idx_value = len(labels)\n",
    "            sources.append(idx_concated_heads)\n",
    "            targets.append(idx_value)\n",
    "            weights.append(defualt_weight)\n",
    "            colors_links.append(positive_activation if is_largest > 0 else negative_activation)\n",
    "            curr_c_proj_meaning = model_aux.hs_to_token_top_k(c_proj[entry_idx], k_top=1, k_bottom=1)\n",
    "            labels.append(curr_c_proj_meaning[case][0])\n",
    "            curr_metadata, curr_color = get_node_customdata(c_proj[entry_idx], prefix=f'value index:{entry_idx} (from head {entry_idx//(model.config.n_embd//model.config.n_head)}), activision:{activision_value}: ', \n",
    "                            top_or_bottom=case, layer_idx=layer_idx, target_word=target_word)  # value is chosen accodring to activation sign. suppouse to reflect the meaning its adding to the output\n",
    "            customdata.append(curr_metadata)\n",
    "            colors_nodes.append(curr_color)\n",
    "            line_explained.append(f'activision:{activision_value}')\n",
    "\n",
    "            # add a link between of the last \"targets\" to the attn_output\n",
    "            sources.append(idx_value)\n",
    "            targets.append(idx_attn_out)\n",
    "            weights.append(abs(activision_mul_norm))\n",
    "            colors_links.append(get_color_according_to_entropy(c_proj[entry_idx]))\n",
    "            line_explained.append(f'activision*norm:{activision_mul_norm}')\n",
    "    \n",
    "    # we also add neurons representing the W_O matrix bias vectors\n",
    "    c_proj_bias = utils.rgetattr(model, f\"transformer.h.{layer_idx}.attn.c_proj.bias\").clone().detach().cpu()\n",
    "    c_proj_bias_meaning = model_aux.hs_to_token_top_k(c_proj_bias, k_top=1, k_bottom=0)\n",
    "    labels.append(c_proj_bias_meaning['top_k'][0])\n",
    "    curr_metadata, curr_color = get_node_customdata(c_proj_bias, prefix='c_proj_bias', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(color_for_bias_vector)\n",
    "    idx_c_proj_bias = len(labels) - 1\n",
    "\n",
    "    # connect the bias vector\n",
    "    sources.append(idx_concated_heads)\n",
    "    targets.append(idx_c_proj_bias)\n",
    "    curr_norm = get_norm_layer(c_proj_bias)\n",
    "    weights.append(defualt_weight)\n",
    "    colors_links.append(color_for_bias_vector)\n",
    "    line_explained.append(f'norm: {curr_norm}')\n",
    "\n",
    "    # connect bias to attn_out\n",
    "    sources.append(idx_c_proj_bias)\n",
    "    targets.append(idx_attn_out)\n",
    "    curr_norm = get_norm_layer(c_proj_bias)\n",
    "    weights.append(curr_norm)\n",
    "    colors_links.append(color_for_bias_vector)\n",
    "    line_explained.append(f'norm: {curr_norm}')\n",
    "\n",
    "    graph_data['idx_attn_part_out'] = idx_attn_output_and_residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_basic_graph(layers, hs_collector, model, model_aux=model_aux, target_word=None, line=None, save_html=False):\n",
    "    '''\n",
    "    A wrapper function to generate a graph for a given layers\n",
    "\n",
    "    @layers: a list of layers to generate the graph for (correctness is guaranteed only if layers are in order)\n",
    "    @ hs_collector: the hs_collector dictionary (created from wrapping the model with the hs_collector class)\n",
    "    @ model: the model (for example: gpt2)\n",
    "    @ model_aux: the model_aux class (more functions for the original model)\n",
    "    @ target_word: the target word for extracting the status of the neurons (ranking and probability)\n",
    "    @ line: the line that was used to generate the data in hs_collector (the prompt to the model)\n",
    "    @ save_html: if True, the graph will be saved as an html file to {title}.html. if @ save_html is a non empty string, the graph will be saved as an html file to {save_html}.html\n",
    "    '''\n",
    "\n",
    "    model = model.cpu() # all the calculations are done on the cpu (also assuming that hs_collector is on the cpu)\n",
    "\n",
    "    # init the graph data\n",
    "    graph_data = {\n",
    "        'sources': [],\n",
    "        'targets': [],\n",
    "        'weights': [],\n",
    "        'colors_nodes': [],\n",
    "        'colors_links': [],\n",
    "        'labels': [],\n",
    "        'line_explained': [],\n",
    "        'customdata': []\n",
    "    }\n",
    "\n",
    "    if type(layers) != list:\n",
    "        layers = [layers]\n",
    "    \n",
    "    # generate the graph for each layer\n",
    "    # each layer is a block of two sub-blocks: the attention block and the mlp block\n",
    "    for layer_idx in layers:\n",
    "        layer_attn_to_graph(layer_idx, graph_data, hs_collector=hs_collector, model=model, model_aux=model_aux, target_word=target_word, line=line)\n",
    "        layer_mlp_to_graph(layer_idx, graph_data, hs_collector=hs_collector, model=model, model_aux=model_aux, target_word=target_word)\n",
    "    \n",
    "    plot_graph_aux(graph_data, title=f'Flow-Grpah of layers {layers}--> propt: \"{line}\". target: \"{target_word}\"', save_html=save_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_basic_graph(10, model=model, hs_collector=hs_collector, target_word=target_token, line=line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example of usage\n",
    "# gen_basic_graph([layer_index for layer_index in range(model_config.n_layer-3, model_config.n_layer)], model=model, hs_collector=hs_collector, target_word=target_token, line=line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # more examples\n",
    "# gen_basic_graph([0], model=model, hs_collector=hs_collector, target_word=target_token, line=line)\n",
    "# gen_basic_graph(8, model=model, hs_collector=hs_collector, target_word=target_token, line=line, save_html='./tmp123.html')\n",
    "# gen_basic_graph([9, 10], model=model, hs_collector=hs_collector, target_word=target_token, line=line, save_html=True)\n",
    "# gen_basic_graph([11], model=model, hs_collector=hs_collector, target_word=target_token, line=line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "march222",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2601361285e90ad390a600e1cfad8ca2ead93725e8d86fc5fa8afa387269ad01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
