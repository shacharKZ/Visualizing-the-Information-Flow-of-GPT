{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "If you read the implementation of our code for gpt2 and want to see its difference with this code,\n",
    "or you want to study it to understand how to do the same with other models, please\n",
    "follow the NOTE-s we left here.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTJForCausalLM\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly express imports\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib as mpl\n",
    "import plotly.io as pio\n",
    "\n",
    "try:\n",
    "    if 'google.colab' in str(get_ipython()):\n",
    "        print('running on colab. plot will be presented in notebook')\n",
    "    else:\n",
    "        # change to \"browser\" if you want to see the plots in your browser, else omit this line\n",
    "        pio.renderers.default = \"browser\"\n",
    "except:\n",
    "    print('Warning: pio.renderers.default = \"browser\" failed. going to use default renderer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another code we wrote\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example 1\n",
    "# line = 'When Mary and John went to the store, John gave a drink to'\n",
    "# target_token = ' Mary' # notice the token includes the space before it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 2\n",
    "line = 'The capital of Japan is the city of'\n",
    "target_token = ' Tokyo' # notice the token includes the space before it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example 3 (from Counterfact)\n",
    "# line = 'Michel Denisot spoke the language of'\n",
    "# target_token = ' French'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # delete the model to free up memory (if more than one model is used)\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model_name = \"EleutherAI/gpt-j-6B\"\n",
    "model = GPTJForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    revision=\"float16\",  # use 16-bit floats to save memory\n",
    ").to(device)\n",
    "\n",
    "model.requires_grad_(False)\n",
    "model_config = model.config\n",
    "\n",
    "# collect the hidden states before and after each of those layers (modules)\n",
    "# <-- NOTE: here we adjust the relevent layers to check. we leave the layers we used for gpt2 for comparison\n",
    "hs_collector = utils.wrap_model(model, layers_to_check = [\n",
    "    '.mlp', '.mlp.fc_in', '.mlp.fc_out', \n",
    "    '.attn', '.attn.q_proj', '.attn.k_proj', '.attn.v_proj', '.attn.out_proj', \n",
    "    '.ln_1', '']) # '' stands for wrapping transformer.h.<layer_index> in gpt2 and gpt-j\n",
    "\n",
    "# hs_collector = utils.wrap_model(model, layers_to_check = [  # gpt2\n",
    "#     '.mlp', '.mlp.c_fc', '.mlp.c_proj',  \n",
    "#     '.attn', '.attn.c_attn', '.attn.c_proj', \n",
    "#     '.ln_1', '.ln_2', '',])  \n",
    "\n",
    "\n",
    "# add extra functions to the model (like logit lens adjust to the model decoding matrix)\n",
    "model_aux = utils.model_extra(model=model, device='cpu')\n",
    "# NOTE for future developers: model_extra obj copy the final layer norm, ln_f, and the decoding matrix, lm_head.\n",
    "# luckily, gpt2 and gpt-j share the same names for these layers\n",
    "# maybe future models will have different names, so we need to check that\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "try:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"  # not blocking, just to prevent warnings and faster tokenization\n",
    "except:\n",
    "    pass\n",
    "encoded_line = tokenizer.encode(line.rstrip(), return_tensors='pt').to(device)\n",
    "output_and_cache = model(encoded_line, output_hidden_states=True, output_attentions=True, use_cache=True)\n",
    "\n",
    "hs_collector['past_key_values'] = output_and_cache.past_key_values  # the \"attentnion memory\"\n",
    "hs_collector['attentions'] = output_and_cache.attentions\n",
    "\n",
    "# the final answer token is:\n",
    "model_answer = tokenizer.decode(output_and_cache.logits[0, -1, :].argmax().item())\n",
    "print(f'model_answer: \"{model_answer}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# const definitions\n",
    "# all the \"factor\" values are used to scale the weights of the links between the neurons in the different layers\n",
    "# you are more than welcome to change these values for your own experiments\n",
    "round_digits = 3\n",
    "\n",
    "# the numbers of top and bottom neurons to show at the mlp matricies (mlp.c_fc, mpl.c_proj) and the attention projection matrix (W_O, attn.c_proj)\n",
    "number_of_top_neurons = 10  \n",
    "number_of_bottom_neurons = 5\n",
    "defualt_weight = 7\n",
    "factor_weight_mlp_key_value_link = 1.5\n",
    "n_values_per_head = 2  # number of ki, vi to show for each head. great to examine with gpt2-small/medium but for gpt2-large/xl you might want to reduce this number\n",
    "factor_attn_score = 10\n",
    "factor_head_output = 1.8\n",
    "factor_head_norm = 0.2\n",
    "factor_for_memory_to_head_values = 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color definitions\n",
    "cmap_node_rank_target = mpl.colors.LinearSegmentedColormap.from_list('cmap_node_rank_target', ['lime', 'greenyellow', 'yellow' , 'yellow', 'yellow'] + ['dimgrey']*53 + ['orangered']*3 + ['red']*4 + ['darkred']*5)\n",
    "cmap_attn_score = mpl.colors.LinearSegmentedColormap.from_list('cmap_attn_score', ['khaki', 'yellow', 'green'])\n",
    "cmap_entropy = mpl.colors.LinearSegmentedColormap.from_list('cmap_entropy', ['darkslategrey', 'lightgrey', 'lightgrey'])\n",
    "\n",
    "backgroud_color = 'black'\n",
    "invisible_link_color = backgroud_color\n",
    "color_for_abstract = 'white'\n",
    "default_color = 'white'\n",
    "link_with_normalizer = 'darkviolet'\n",
    "color_for_bias_vector = 'pink'\n",
    "positive_activation = 'blue'\n",
    "negative_activation = 'red'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_layer(x, debug_msg=None, round_digits=round_digits):\n",
    "    if type(x) == torch.Tensor:\n",
    "        res = torch.norm(x).item()\n",
    "    else:\n",
    "        res = torch.norm(torch.Tensor(x)).item()\n",
    "    if debug_msg:\n",
    "        print(f'{debug_msg}: len(x): {len(x)}, norm: {res}')\n",
    "    if round_digits > 0:\n",
    "        return round(res, round_digits)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph_aux(graph_data, title=f'Flow-Graph', save_html=False):\n",
    "    '''\n",
    "    A wrapper for graph plotting by plotly express\n",
    "\n",
    "    @ graph_data: the graph data. see the function @ gen_basic_graph for more details\n",
    "    @ title: the title of the graph\n",
    "    @ save_html: if True, the graph will be saved as an html file to {title}.html. if @ save_html is a non empty string, the graph will be saved as an html file to {save_html}.html\n",
    "    '''\n",
    "    \n",
    "    sources = graph_data['sources']\n",
    "    targets = graph_data['targets']\n",
    "    weights = graph_data['weights']\n",
    "    colors_nodes = graph_data['colors_nodes']\n",
    "    colors_links = graph_data['colors_links']\n",
    "    labels = graph_data['labels']\n",
    "    line_explained = graph_data['line_explained']\n",
    "    customdata = graph_data['customdata']\n",
    "\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "      valueformat = \".0f\",\n",
    "      valuesuffix = \"TWh\",\n",
    "      node = dict(\n",
    "        pad = 15,\n",
    "        thickness = 15,\n",
    "        line = dict(color = backgroud_color, width = 0.5),\n",
    "        label = labels,\n",
    "        color = colors_nodes,\n",
    "        customdata = customdata,\n",
    "        hovertemplate='Node: %{customdata}. %{value}<extra></extra>',\n",
    "      ),\n",
    "      link = dict(\n",
    "        source =  sources,\n",
    "        target =  targets,\n",
    "        value =  weights,\n",
    "        color = colors_links,\n",
    "        customdata = line_explained,\n",
    "        hovertemplate='%{source.customdata}<br />' + ' '*50 + '----[%{customdata},  %{value}]----><br />%{target.customdata}<extra></extra>',\n",
    "      ))]\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        hovermode = 'x',\n",
    "        title_text = title,\n",
    "        font=dict(size = 10, color = 'white'),\n",
    "        plot_bgcolor=backgroud_color,\n",
    "        paper_bgcolor=backgroud_color\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # save to html\n",
    "    if save_html != False:\n",
    "      path_out = f'{title}.html' if (type(save_html) != str or save_html == '') else f'{save_html}.html'\n",
    "      fig.write_html(path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_f = copy.deepcopy(utils.rgetattr(model, 'transformer.ln_f')).to(device).requires_grad_(False)\n",
    "decoding_matrix = copy.deepcopy(utils.rgetattr(model, 'lm_head')).to(device).requires_grad_(False)\n",
    "hidden_d = model.config.n_embd\n",
    "\n",
    "def logit_status(hs, wanted_idx):\n",
    "        if wanted_idx == '':\n",
    "                return -1, -1\n",
    "        if type(wanted_idx) == str:\n",
    "                wanted_idx = tokenizer.encode(wanted_idx, add_special_tokens=False)[0]\n",
    "        if type(hs) == torch.Tensor:\n",
    "                hs_tensor = hs.clone().detach().to(device)\n",
    "        else:\n",
    "                hs_tensor = torch.tensor(hs).to(device)\n",
    "        \n",
    "        # logit len including layer normaization with the model final layer norm (ln final)\n",
    "        specific_logics = ln_f(hs_tensor)\n",
    "        # hs to vocab scores\n",
    "        specific_logic_lens = decoding_matrix(specific_logics)\n",
    "        # scores to probabilities\n",
    "        specific_logics = torch.nn.functional.softmax(specific_logic_lens, dim=0)\n",
    "\n",
    "        # get probability of the wanted_idx\n",
    "        prob = specific_logics[wanted_idx].item()\n",
    "        # get ranking of the wanted_idx\n",
    "        smaller = torch.where(specific_logic_lens[wanted_idx] < specific_logic_lens)[0].size()[0]\n",
    "        ranking = smaller+1\n",
    "        # rank 1 -> most probable, rank #vocab_size -> least probable\n",
    "        return prob, ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(probabilities):\n",
    "    # calculates the entropy of a probability distribution\n",
    "    if len(probabilities) != model_config.vocab_size:\n",
    "        probabilities = model_aux.hs_to_probs(probabilities)\n",
    "    # convert the probabilities to a numpy array\n",
    "    probabilities = np.array(probabilities.detach())\n",
    "    # filter out 0 probabilities (to avoid issues with log(0))\n",
    "    non_zero_probs = probabilities[probabilities != 0]\n",
    "    entropy = -np.sum(non_zero_probs * np.log2(non_zero_probs))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_according_to_entropy(hs, max_val=30):\n",
    "    entropy_score = entropy(hs)\n",
    "    color_idx = entropy_score / max_val\n",
    "    color_idx = max(min(color_idx, 1), 0)\n",
    "    color = f'rgba{cmap_entropy(color_idx)}'\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_customdata(hs, prefix='', top_or_bottom='top_k', layer_idx=None, target_word=None, color_flag=True):\n",
    "    '''\n",
    "    return metadata that uses in the graph plot\n",
    "    text: the text that will be displayed in the node when hovering over it\n",
    "    color: the color of the node accodring to its probability of the target word \n",
    "    (green if very probable, red if very improbable and grey otherwise)\n",
    "    '''\n",
    "\n",
    "    color = default_color \n",
    "    hs_meaning = model_aux.hs_to_token_top_k(hs, k_top=5, k_bottom=5)\n",
    "    res = f'{hs_meaning[top_or_bottom]}'\n",
    "\n",
    "    if prefix != '':\n",
    "        res = f'{prefix}: {res}'\n",
    "    if layer_idx is not None:\n",
    "        res = f'{layer_idx}) {res}'\n",
    "    if target_word is not None:\n",
    "        prob, ranking = logit_status(hs, target_word)\n",
    "        prob = round(prob*100, round_digits)  # probs [0,1] as percentage [0,100]\n",
    "        res = f'{res} [status: \"{target_word}\": prob:{prob}%, rank: {ranking})]'\n",
    "        if color_flag:\n",
    "            color_idx = ranking/model_config.vocab_size  # color index [0,1] (according to ranking)\n",
    "            color = f'rgba{cmap_node_rank_target(color_idx)}'\n",
    "    return res, color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE:\n",
    "The main difference between gpt2 and gpt-j is that in gpt2 each block is an attention block followed by a mlp (feed-forward, FF) \n",
    "block, however in gpt-j the attention and the mlp block are parrallel to each other. Also:\n",
    "* where in gpt2 we have a residual for each of the attention and mlp blocks, in gpt-j we have only single \n",
    "residual for the whole block. \n",
    "* gpt-j adds the position embedding to each block, while gpt-2 does that only to the first block. we did not add it to the graph \n",
    "to keep it simple to the gpt2 graph.\n",
    "\n",
    "except for the differences mentioned above, the structure of the models and their graph is mostly the same.\n",
    "we use the same code we used for gpt2 except for the following changes:\n",
    "- adjusting names of the model parameters (for example, in gpt2 mlp matrix called 'c_fc' and 'c_proj' while in gpt-j it is called 'fc_in' and 'fc_out')\n",
    "- addiing an extra function for the creation of the sub-graphs of the attentnion and mlp sub-blocks: pre_connect_mlp_and_attn_for_gptj\n",
    "this function create the input for each sub-block and the common output. each sub-block connect to the input and the common output.\n",
    "this structer helps us to use mostly the same code for both gpt2 and gpt-j.\n",
    "'''\n",
    "def pre_connect_mlp_and_attn_for_gptj(layer_idx: int, graph_data, model, hs_collector, row_idx=-1, model_aux=model_aux, target_word=None):\n",
    "      sources = graph_data['sources']\n",
    "      targets = graph_data['targets']\n",
    "      weights = graph_data['weights']\n",
    "      colors_nodes = graph_data['colors_nodes']\n",
    "      colors_links = graph_data['colors_links']\n",
    "      labels = graph_data['labels']\n",
    "      line_explained = graph_data['line_explained']\n",
    "      customdata = graph_data['customdata']\n",
    "\n",
    "      if 'idx_previous_block' in graph_data:\n",
    "            idx_residual_and_input = graph_data['idx_previous_block']\n",
    "            residual_and_input_norm = graph_data['previous_block_norm']  \n",
    "      else:\n",
    "            block_residual = hs_collector[layer_idx]['ln_1']['input'][row_idx]  # block_residual == previous layer output\n",
    "            block_residual_meaning = model_aux.hs_to_token_top_k(block_residual, k_top=1, k_bottom=0)\n",
    "            labels.append(block_residual_meaning['top_k'][0])\n",
    "            curr_metadata, curr_color = get_node_customdata(block_residual, prefix='mlp_input', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "            customdata.append(curr_metadata)\n",
    "            colors_nodes.append(curr_color)\n",
    "            residual_and_input_norm = get_norm_layer(block_residual)\n",
    "            idx_residual_and_input = len(labels) - 1\n",
    "\n",
    "      block_out = hs_collector[layer_idx]['']['output'][row_idx]  # mlp_out + attention_out + attn_out + residual\n",
    "      block_out_meaning = model_aux.hs_to_token_top_k(block_out, k_top=1, k_bottom=0)\n",
    "      labels.append(block_out_meaning['top_k'][0])\n",
    "      curr_metadata, curr_color = get_node_customdata(block_out, prefix='block_output', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "      customdata.append(curr_metadata)\n",
    "      colors_nodes.append(curr_color)\n",
    "      idx_curr_block_out = len(labels) - 1\n",
    "      curr_block_out_norm = get_norm_layer(block_out)\n",
    "\n",
    "      # connect the residual to the block output\n",
    "      sources.append(idx_residual_and_input)\n",
    "      targets.append(idx_curr_block_out)\n",
    "      weights.append(residual_and_input_norm)\n",
    "      colors_links.append('rgba(102,0,204,0.3)')  # unique color for the residual\n",
    "      line_explained.append(f'residual + mlp_out + attn_out, norm: {residual_and_input_norm}')\n",
    "\n",
    "      # create the input to the attn and mlp block (the norm of the residual)\n",
    "      block_input = hs_collector[layer_idx]['ln_1']['output'][row_idx]  # block_input == norm of the residual\n",
    "      block_input_meaning = model_aux.hs_to_token_top_k(block_input, k_top=1, k_bottom=0)\n",
    "      labels.append(block_input_meaning['top_k'][0])\n",
    "      curr_metadata, curr_color = get_node_customdata(block_input, prefix='blocks_input (after ln_1)', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "      customdata.append(curr_metadata)\n",
    "      colors_nodes.append(curr_color)\n",
    "      idx_curr_blocks_input = len(labels) - 1\n",
    "      curr_blocks_input_norm = get_norm_layer(block_input)\n",
    "\n",
    "      # connect the residual to the blocks input\n",
    "      sources.append(idx_residual_and_input)\n",
    "      targets.append(idx_curr_blocks_input)\n",
    "      weights.append(residual_and_input_norm)\n",
    "      colors_links.append('rgba(102,0,204,0.3)')  # unique color for the residual\n",
    "      line_explained.append(f'residual into layer norm, norm: {residual_and_input_norm}')\n",
    "\n",
    "      # save the common input of the sub-blocks and the common outputs\n",
    "      # so each function for the creation of the sub-blocks can connect to them\n",
    "      graph_data['idx_previous_block'] = idx_residual_and_input\n",
    "      graph_data['idx_curr_block_out'] = idx_curr_block_out\n",
    "      graph_data['curr_block_out_norm'] = curr_block_out_norm\n",
    "\n",
    "      graph_data['idx_curr_blocks_input'] = idx_curr_blocks_input\n",
    "      graph_data['curr_blocks_input_norm'] = curr_blocks_input_norm\n",
    "      \n",
    "      # we already prepared the input to the next block (for the call of this function with {layer_idx+1})\n",
    "      graph_data['idx_previous_block'] = idx_curr_block_out\n",
    "      graph_data['previous_block_norm'] = curr_block_out_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_mlp_to_graph(layer_idx: int, graph_data, model, hs_collector, row_idx=-1, model_aux=model_aux, target_word=None):\n",
    "    '''\n",
    "    create a subgraph of the feed-forward (FF, MLP) part of the model at layer_idx\n",
    "    the subgraph is a graph of the neurons in the FF part of the model\n",
    "    the nodes are the most active neurons in the FF (some of positive and some of negative)\n",
    "    the links are the connections between the neurons (summation of the neurons or when one neuron creates the coefficient of another neuron)\n",
    "    the graph is created using the graph_data dictionary\n",
    "    if the graph_data dictionary's lists are empty, they will be initialized\n",
    "    if the graph_data dictionary 's lists are not empty, they will be updated (try to connect the new nodes to the existing nodes)\n",
    "\n",
    "    @ layer_idx: the index of the layer in the model\n",
    "    @ graph_data: the graph data dictionary (if called first time, should include empty list for the keys it uses)\n",
    "    @ model: the model (for example: gpt2)\n",
    "    @ hs_collector: the hs_collector dictionary (created from wrapping the model with the hs_collector class)\n",
    "    @ row_idx: the index of the row in the hs_collector which correspond to the infrence of the #row_idx token. use -1 for the last token (Note: currently not supported any other value than -1)\n",
    "    @ model_aux: the model_aux class (more functions for the original model)\n",
    "    @ target_word: the target word for extracting the status of the neurons (ranking and probability)    \n",
    "    '''\n",
    "    sources = graph_data['sources']\n",
    "    targets = graph_data['targets']\n",
    "    weights = graph_data['weights']\n",
    "    colors_nodes = graph_data['colors_nodes']\n",
    "    colors_links = graph_data['colors_links']\n",
    "    labels = graph_data['labels']\n",
    "    line_explained = graph_data['line_explained']\n",
    "    customdata = graph_data['customdata']\n",
    "\n",
    "    # <-- NOTE: assuming the input and output of this block was just created using pre_connect_mlp_and_attn_for_gptj\n",
    "    # in gpt-j the mlp and attentnion are parallel and not sequential as in gpt2\n",
    "    # for this case, those nodes are create separately in the pre_connect_mlp_and_attn_for_gptj function\n",
    "    # and now we access their indices from the graph_data dictionary\n",
    "    idx_curr_block_out = graph_data['idx_curr_block_out']\n",
    "    idx_curr_blocks_input = graph_data['idx_curr_blocks_input']\n",
    "    curr_blocks_input_norm = graph_data['curr_blocks_input_norm']\n",
    "    # mlp_residual = hs_collector[layer_idx]['ln_1']['input'][row_idx]  # gpt2. left for reference\n",
    "\n",
    "    mlp_input = hs_collector[layer_idx]['mlp']['input'][row_idx]  # mlp_input == c_fc_input\n",
    "    mlp_out = hs_collector[layer_idx]['mlp']['output'][row_idx]  # only the output of the mlp\n",
    "\n",
    "    mlp_input_meaning = model_aux.hs_to_token_top_k(mlp_input, k_top=1, k_bottom=0)\n",
    "    mlp_out_meaning = model_aux.hs_to_token_top_k(mlp_out, k_top=1, k_bottom=0)\n",
    "\n",
    "    labels.append(mlp_input_meaning['top_k'][0])\n",
    "    curr_metadata, curr_color = get_node_customdata(mlp_input, prefix='mlp_input', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(curr_color)\n",
    "    idx_mlp_input = len(labels) - 1\n",
    "\n",
    "    labels.append(mlp_out_meaning['top_k'][0])\n",
    "    curr_metadata, curr_color = get_node_customdata(mlp_out, prefix='mlp_out', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(curr_color)\n",
    "    idx_mlp_out = len(labels) - 1\n",
    "\n",
    "    sources.append(idx_curr_blocks_input)\n",
    "    targets.append(idx_mlp_input)\n",
    "    weights.append(curr_blocks_input_norm)\n",
    "    colors_links.append(link_with_normalizer)\n",
    "    line_explained.append(f'mlp input after norm:{curr_blocks_input_norm}')\n",
    "\n",
    "    sources.append(idx_mlp_out)\n",
    "    targets.append(idx_curr_block_out)  # TODO\n",
    "    curr_norm = get_norm_layer(mlp_out)\n",
    "    weights.append(curr_norm)\n",
    "    colors_links.append(get_color_according_to_entropy(mlp_out))\n",
    "    line_explained.append(f'norm:{curr_norm}')\n",
    "\n",
    "    # get the first and second matricies of the mlp\n",
    "    c_fc = utils.rgetattr(model, f\"transformer.h.{layer_idx}.mlp.fc_in.weight\").clone().detach().cpu().T  # <-- NOTE: adjust this layer name\n",
    "    c_proj = utils.rgetattr(model, f\"transformer.h.{layer_idx}.mlp.fc_out.weight\").clone().detach().cpu().T  # <-- NOTE: adjust this layer name\n",
    "    \n",
    "    # NOTE: for better understanding, we left the code we used for GPT-2\n",
    "    # c_fc = utils.rgetattr(model, f\"transformer.h.{layer_idx}.mlp.c_fc.weight\").clone().detach().cpu()\n",
    "    # c_proj = utils.rgetattr(model, f\"transformer.h.{layer_idx}.mlp.c_proj.weight\").clone().detach().cpu()\n",
    "\n",
    "    # you can change this according to your needs (like screen size)\n",
    "\n",
    "    values_norm = c_proj.norm(dim=1)\n",
    "    hs =  hs_collector[layer_idx]['mlp.fc_out']['input'][row_idx]  # value activation. mid results between the key and value matrix\n",
    "    hs_mul_norm = hs * values_norm  # this is our metric for the importance of the value activation (value*norm of the second matrix)\n",
    "    \n",
    "    # pick the top most activate neurons (according to activasion sign)\n",
    "    for case, n_top, is_largest in [('top_k', number_of_top_neurons, True), ('bottom_k', number_of_bottom_neurons, False)]:\n",
    "        tops = torch.topk(hs_mul_norm, k=n_top, largest=is_largest)\n",
    "        for entry_idx, activision_value_mul_norm in zip(tops.indices, tops.values):\n",
    "            activision_value_mul_norm = round(activision_value_mul_norm.item(), round_digits)\n",
    "            activision_value = round(hs[entry_idx].item(), round_digits)\n",
    "            entry_idx = entry_idx.item()\n",
    "\n",
    "            # create a node for the \"key\" neuron (the first matrix)\n",
    "            idx_key = len(labels)\n",
    "            idx_value = idx_key+1\n",
    "            curr_c_fc_meaning = model_aux.hs_to_token_top_k(c_fc.T[entry_idx], k_top=1, k_bottom=0)\n",
    "            labels.append(curr_c_fc_meaning['top_k'][0])\n",
    "            curr_metadata, curr_color = get_node_customdata(c_fc.T[entry_idx], prefix=f'key{entry_idx}', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "            customdata.append(curr_metadata)\n",
    "            colors_nodes.append(curr_color)\n",
    "\n",
    "            # create a node for the \"value\" neuron (the second matrix)\n",
    "            curr_c_proj_meaning = model_aux.hs_to_token_top_k(c_proj[entry_idx], k_top=1, k_bottom=1)\n",
    "            labels.append(curr_c_proj_meaning[case][0])  # value is chosen accodring to activation sign. suppouse to reflect the meaning its adding to the output\n",
    "            curr_metadata, curr_color = get_node_customdata(c_proj[entry_idx], prefix=f'value{entry_idx}', top_or_bottom=case, layer_idx=layer_idx, target_word=target_word)\n",
    "            customdata.append(curr_metadata)\n",
    "            colors_nodes.append(curr_color)\n",
    "\n",
    "            # connect the \"key\" and the \"value\"\n",
    "            sources.append(idx_key)\n",
    "            targets.append(idx_value)\n",
    "            weights.append(abs(activision_value)*factor_weight_mlp_key_value_link)\n",
    "            colors_links.append(positive_activation if is_largest else negative_activation)\n",
    "            line_explained.append(f'activision:{activision_value}')\n",
    "\n",
    "            # add between the mlp_input and the \"key\" neuron\n",
    "            sources.append(idx_mlp_input)\n",
    "            targets.append(idx_key)\n",
    "            weights.append(defualt_weight)\n",
    "            colors_links.append(get_color_according_to_entropy(c_fc.T[entry_idx]))\n",
    "            line_explained.append(f'')\n",
    "\n",
    "            # add a link between of the \"value\" neuron to the mlp_out\n",
    "            sources.append(idx_value)\n",
    "            targets.append(idx_mlp_out)\n",
    "            weights.append(abs(activision_value_mul_norm))\n",
    "            colors_links.append(get_color_according_to_entropy(c_proj[entry_idx]))\n",
    "            line_explained.append(f'activision*norm:{abs(activision_value_mul_norm)}')\n",
    "    \n",
    "    # we also add neurons representing the matricies bias vectors\n",
    "    # we create nodes for each matricies bias vector then connect them to the flow\n",
    "    c_fc_bias = utils.rgetattr(model, f\"transformer.h.{layer_idx}.mlp.fc_in.bias\").clone().detach().cpu() @ c_proj  # NOTE <-- the first mlp matrix bias name\n",
    "    c_fc_bias_meaning = model_aux.hs_to_token_top_k(c_fc_bias, k_top=1, k_bottom=0)\n",
    "    labels.append(c_fc_bias_meaning['top_k'][0])\n",
    "    curr_metadata, curr_color = get_node_customdata(c_fc_bias, prefix='fc_out_bias', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(color_for_bias_vector) # special color for bias\n",
    "    idx_c_fc_bias = len(labels) - 1\n",
    "\n",
    "    c_proj_bias = utils.rgetattr(model, f\"transformer.h.{layer_idx}.mlp.fc_out.bias\").clone().detach().cpu()  # NOTE <-- the second mlp matrix bias name\n",
    "    c_proj_bias_meaning = model_aux.hs_to_token_top_k(c_proj_bias, k_top=1, k_bottom=0)\n",
    "    labels.append(c_proj_bias_meaning['top_k'][0])\n",
    "    curr_metadata, curr_color = get_node_customdata(c_proj_bias, prefix='fc_out_bias', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(color_for_bias_vector)\n",
    "    idx_c_proj_bias = len(labels) - 1\n",
    "\n",
    "    # connect mlp_input to c_fc_bias\n",
    "    sources.append(idx_mlp_input)\n",
    "    targets.append(idx_c_fc_bias)\n",
    "    curr_norm = get_norm_layer(mlp_input)\n",
    "    weights.append(defualt_weight)\n",
    "    colors_links.append(color_for_bias_vector)\n",
    "    line_explained.append(f'norm:{curr_norm}')\n",
    "\n",
    "    # connect c_fc_bias to c_proj_bias, although it is not really a link (this why its color is invisible_link_color)\n",
    "    sources.append(idx_c_fc_bias)\n",
    "    targets.append(idx_c_proj_bias)\n",
    "    weights.append(0.05) # to be barely visible (trick to make it unvisible with the background)\n",
    "    colors_links.append(invisible_link_color)\n",
    "    line_explained.append(f'norm:{curr_norm}')\n",
    "\n",
    "    # connect c_proj_bias to mlp_out\n",
    "    sources.append(idx_c_proj_bias)\n",
    "    targets.append(idx_mlp_out)\n",
    "    curr_norm = get_norm_layer(c_proj_bias)\n",
    "    weights.append(curr_norm)\n",
    "    colors_links.append(color_for_bias_vector)\n",
    "    line_explained.append(f'norm:{curr_norm}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_attn_to_graph(layer_idx, graph_data, hs_collector, model, row_idx=-1, model_aux=model_aux, target_word=None, line=None):\n",
    "    '''\n",
    "    create a graph for the attention (attn) layer\n",
    "    the subgraph is a graph of the neurons in the Q, K, V O matricies, mostly aggregated into heads\n",
    "    the nodes are single or small groups of neurons (when they are aggregated into heads or subheads)\n",
    "    the links are the connections between the neurons (summation of the neurons or when one neuron creates the coefficient of another neuron)\n",
    "    the graph is created using the graph_data dictionary\n",
    "    if the graph_data dictionary's lists are empty, they will be initialized\n",
    "    if the graph_data dictionary 's lists are not empty, they will be updated (try to connect the new nodes to the existing nodes)\n",
    "\n",
    "    @ layer_idx: the index of the layer in the model\n",
    "    @ graph_data: the graph data dictionary (if called first time, should include empty list for the keys it uses)\n",
    "    @ model: the model (for example: gpt2)\n",
    "    @ hs_collector: the hs_collector dictionary (created from wrapping the model with the hs_collector class)\n",
    "    @ row_idx: the index of the row in the hs_collector which correspond to the infrence of the #row_idx token. use -1 for the last token (Note: currently not supported any other value than -1)\n",
    "    @ model_aux: the model_aux class (more functions for the original model)\n",
    "    @ target_word: the target word for extracting the status of the neurons (ranking and probability)\n",
    "    @ line: the line that was used to generate the data in hs_collector (the prompt to the model)\n",
    "    '''\n",
    "    sources = graph_data['sources']\n",
    "    targets = graph_data['targets']\n",
    "    weights = graph_data['weights']\n",
    "    colors_nodes = graph_data['colors_nodes']\n",
    "    colors_links = graph_data['colors_links']\n",
    "    labels = graph_data['labels']\n",
    "    line_explained = graph_data['line_explained']\n",
    "    customdata = graph_data['customdata']\n",
    "\n",
    "    # <-- NOTE: assuming the input and output of this block was just created using pre_connect_mlp_and_attn_for_gptj\n",
    "    idx_curr_block_out = graph_data['idx_curr_block_out']\n",
    "    idx_curr_blocks_input = graph_data['idx_curr_blocks_input']\n",
    "    curr_blocks_input_norm = graph_data['curr_blocks_input_norm']\n",
    "\n",
    "    # uses to show what was the token that generated the attention memory (previous keys and layer)\n",
    "    # the i-th key and i-th value were generated by the i-th token\n",
    "    # if @line is not given (None) - will not show this information\n",
    "    parsed_line = None\n",
    "    if line is not None:\n",
    "        parsed_line = tokenizer.encode(line, return_tensors='pt')\n",
    "        # save the parsed line for later use\n",
    "        parsed_line = [tokenizer.decode(x.item()) for x in parsed_line[0]]\n",
    "\n",
    "\n",
    "    # create nodes for the attention input (after layer norm) and for the attention output\n",
    "    attn_input = hs_collector[layer_idx]['attn']['input'][row_idx]\n",
    "    attn_input_meaning = model_aux.hs_to_token_top_k(attn_input, k_top=1, k_bottom=0)\n",
    "    labels.append(attn_input_meaning['top_k'][0])\n",
    "    curr_metadata, curr_color = get_node_customdata(attn_input, prefix=f'attn_input', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(curr_color)\n",
    "    idx_attn_input = len(labels) - 1\n",
    "\n",
    "    # attn output\n",
    "    attn_out = hs_collector[layer_idx]['attn']['output'][row_idx]\n",
    "    attn_out_meaning = model_aux.hs_to_token_top_k(attn_out, k_top=1, k_bottom=0)\n",
    "    labels.append(attn_out_meaning['top_k'][0])\n",
    "    curr_metadata, curr_color = get_node_customdata(attn_out, prefix=f'attn_out', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(curr_color)\n",
    "    idx_attn_out = len(labels) - 1\n",
    "\n",
    "    # link between the common input and attn_input\n",
    "    sources.append(idx_curr_blocks_input)\n",
    "    targets.append(idx_attn_input)\n",
    "    weights.append(curr_blocks_input_norm) \n",
    "    colors_links.append(link_with_normalizer)\n",
    "    line_explained.append(f'attn input after norm:{curr_blocks_input_norm}')\n",
    "\n",
    "    # link between attn_output and idx_curr_block_out\n",
    "    sources.append(idx_attn_out)\n",
    "    targets.append(idx_curr_block_out)\n",
    "    curr_weight = get_norm_layer(attn_out)\n",
    "    weights.append(curr_weight)\n",
    "    colors_links.append(get_color_according_to_entropy(attn_out))\n",
    "    line_explained.append(f'norm:{curr_weight}')\n",
    "\n",
    "    # <-- NOTE: adjust the access to the attentnion matrices \n",
    "    Wq = utils.rgetattr(model, f\"transformer.h.{layer_idx}.attn.q_proj.weight\").clone().detach().cpu()\n",
    "    Wk = utils.rgetattr(model, f\"transformer.h.{layer_idx}.attn.k_proj.weight\").clone().detach().cpu()\n",
    "    Wv = utils.rgetattr(model, f\"transformer.h.{layer_idx}.attn.k_proj.weight\").clone().detach().cpu()\n",
    "    c_proj = utils.rgetattr(model, f\"transformer.h.{layer_idx}.attn.out_proj.weight\").clone().detach().cpu().T\n",
    "\n",
    "    # NOTE: we leave the code for the original gpt2 model (there, Q,K,V matrices are actually concatenated into one matrix)\n",
    "    # c_attn = utils.rgetattr(model, f\"transformer.h.{layer_idx}.attn.c_attn.weight\").clone().detach().cpu() # W_QKV (the QKV matrix)\n",
    "    # c_proj = utils.rgetattr(model, f\"transformer.h.{layer_idx}.attn.c_proj.weight\").clone().detach().cpu() # W_O (the Output/projection matrix)\n",
    "    # # in gpt2 c_attn is the concatenation of Wq, Wk, Wv (Q, K, V)\n",
    "    # Wq = c_attn[:, :hidden_d]\n",
    "    # Wk = c_attn[:, hidden_d:2*hidden_d]\n",
    "    # Wv = c_attn[:, 2*hidden_d:]\n",
    "\n",
    "    # <-- NOTE: adjust the access to the hidden states\n",
    "    # NOTE: we leave the code for the original gpt2 model under each of the q,k,v hidden states\n",
    "    q = hs_collector[layer_idx]['attn.q_proj']['output'][row_idx].cpu()  # <-- NOTE (this layer query)\n",
    "    # q = c_attn_output[ :hidden_d]  # gpt2\n",
    "    k = hs_collector[layer_idx]['attn.k_proj']['output'][row_idx].cpu()  # <-- NOTE (this layer key. it is added to the attention memory (to \"past_key_values\" so also the next tokens can use it) )\n",
    "    # k = c_attn_output[hidden_d:2*hidden_d]  # gpt2\n",
    "    v = hs_collector[layer_idx]['attn.v_proj']['output'][row_idx].cpu()  # <-- NOTE (this layer value. like the key, it is added to the attention memory )\n",
    "    # v = c_attn_output[2*hidden_d:] # gpt2\n",
    "\n",
    "    # projection using the QK circuit\n",
    "    def pre_project_q(hs_q):\n",
    "        return hs_q @ Wk\n",
    "        # return Wk @ hs_q\n",
    "\n",
    "    # projection using the QK circuit\n",
    "    def pre_project_k(hs_k):\n",
    "        # return hs_k @ Wq\n",
    "        return Wq @ hs_k\n",
    "    \n",
    "    # projection using the OV circuit\n",
    "    def pre_project_v(hs_v):\n",
    "        # return hs_v @ c_proj\n",
    "        return hs_v @ c_proj\n",
    "    \n",
    "    q_projected = pre_project_q(q)\n",
    "    k_projected = pre_project_k(k)\n",
    "    v_poject = pre_project_v(v)  \n",
    "\n",
    "    # create a node for q,k,v together\n",
    "    q_meaning = model_aux.hs_to_token_top_k(q_projected, k_top=1, k_bottom=0)\n",
    "    q_data, curr_color = get_node_customdata(q_projected, prefix=f'q (for current calc)', top_or_bottom='top_k', target_word=target_word)\n",
    "    k_data, _ = get_node_customdata(k_projected, prefix=f'k (for next tokens)', top_or_bottom='top_k', target_word=target_word)\n",
    "    v_data, _ = get_node_customdata(v_poject, prefix=f'v (for next tokens)', top_or_bottom='top_k', target_word=target_word)\n",
    "\n",
    "    curr_metadata = f'q,k,v (before splitting into heads):' + '<br />' + q_data + '<br />' + k_data + '<br />' + v_data\n",
    "\n",
    "    # create a new node for query (q) and add the key and value (k, v) metadata. we call this node qkv_full\n",
    "    labels.append(q_meaning['top_k'][0])\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(curr_color)\n",
    "    idx_qkv_full = len(labels) - 1\n",
    "\n",
    "    # connect between attn_input and qkv_full   \n",
    "    sources.append(idx_attn_input)\n",
    "    targets.append(idx_qkv_full)\n",
    "    curr_weight = get_norm_layer(attn_input)\n",
    "    weights.append(curr_weight)\n",
    "    colors_links.append(get_color_according_to_entropy(q))\n",
    "    line_explained.append(f'norm:{curr_weight}')\n",
    "\n",
    "    # the concated results from all the heads but without the OV circuit projection\n",
    "    concated_heads_wihtout_projection = hs_collector[layer_idx]['attn.out_proj']['input'][row_idx]  # <-- NOTE: adjust layer name\n",
    "    # concated_heads_wihtout_projection = hs_collector[layer_idx]['attn.c_proj']['input'][row_idx] # gpt2\n",
    "\n",
    "    idx_concated_heads = len(labels)  # attn_c_proj input\n",
    "    concated_heads_without_projection_meaning = model_aux.hs_to_token_top_k(concated_heads_wihtout_projection, k_top=1, k_bottom=0)\n",
    "    labels.append(concated_heads_without_projection_meaning['top_k'][0])\n",
    "    curr_metadata, curr_color = get_node_customdata(concated_heads_wihtout_projection, prefix=f'concated_heads (without projection)', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "    customdata.append(curr_metadata)\n",
    "    colors_nodes.append(color_for_abstract)\n",
    "\n",
    "    # print the hidden meaning of each of the heads\n",
    "    dim_head = model.config.n_embd // model.config.n_head\n",
    "\n",
    "    # create for each head the following nodes:\n",
    "    # (1) the qi - this head part in the query q (we also add its information about ki, vi that were generated at this layer and saved to the attention memory)\n",
    "    # (2) its #n_values_per_head top biggest ki and vi (the keys and values from the attention memory) accodring to the attention score\n",
    "    # (3) the head output - the weighted summation of all the vi into it \n",
    "    for head_idx in range(model.config.n_head):\n",
    "        # hs_collector['past_key_values'][layer_idx][0 for key, 1 for value][entry in batch][head_idx] -> list of the keys/values for this head. the i-th entry is the key/value for the i-th token in the input\n",
    "        keys = hs_collector['past_key_values'][layer_idx][0][0][head_idx]  \n",
    "        values = hs_collector['past_key_values'][layer_idx][1][0][head_idx]\n",
    "        attentions = hs_collector['attentions'][layer_idx][0][head_idx][row_idx]\n",
    "\n",
    "        # qi with the information about ki, vi (1)\n",
    "        qi = q[dim_head * head_idx: dim_head * (head_idx + 1)]\n",
    "        ki = k[dim_head * head_idx: dim_head * (head_idx + 1)]\n",
    "        vi = v[dim_head * head_idx: dim_head * (head_idx + 1)]\n",
    "\n",
    "        q_i_fill = torch.zeros(model.config.n_embd)\n",
    "        q_i_fill[dim_head*head_idx:dim_head*(head_idx+1)] = qi\n",
    "        q_i_projected = pre_project_q(q_i_fill)\n",
    "\n",
    "        k_i_fill = torch.zeros(model.config.n_embd)\n",
    "        k_i_fill[dim_head*head_idx:dim_head*(head_idx+1)] = ki\n",
    "        k_i_projected = pre_project_k(k_i_fill)\n",
    "\n",
    "        v_i_fill = torch.zeros(model.config.n_embd)\n",
    "        v_i_fill[dim_head*head_idx:dim_head*(head_idx+1)] = vi\n",
    "        v_i_projected = pre_project_v(v_i_fill) \n",
    "\n",
    "        q_i_projected_meaning = model_aux.hs_to_token_top_k(q_i_projected, k_top=1, k_bottom=0)\n",
    "        q_data, curr_color = get_node_customdata(q_i_projected, prefix=f'qi (for current calc)', top_or_bottom='top_k', target_word=target_word)\n",
    "        k_data, _ = get_node_customdata(k_i_projected, prefix=f'ki (for next tokens)', top_or_bottom='top_k', target_word=target_word)\n",
    "        v_data, _ = get_node_customdata(v_i_projected, prefix=f'vi (for next tokens)', top_or_bottom='top_k', target_word=target_word)\n",
    "\n",
    "        curr_metadata = f'head {head_idx}:' + '<br />' + q_data + '<br />' + k_data + '<br />' + v_data\n",
    "\n",
    "        # create a new node for query and add the key and value metadata\n",
    "        labels.append(q_i_projected_meaning['top_k'][0])\n",
    "        customdata.append(curr_metadata)\n",
    "        colors_nodes.append(curr_color)\n",
    "        idx_q_i_projected = len(labels) - 1\n",
    "\n",
    "        # create a link between the input and the head query \n",
    "        sources.append(idx_qkv_full)\n",
    "        targets.append(idx_q_i_projected)\n",
    "        curr_weight = get_norm_layer(q_i_projected)\n",
    "        weights.append(curr_weight*factor_head_norm)\n",
    "        colors_links.append(get_color_according_to_entropy(q_i_projected))\n",
    "        line_explained.append(f'norm: {curr_weight}')\n",
    "\n",
    "        # create a new node for head output (3)\n",
    "        head_output = torch.zeros(model.config.n_embd)\n",
    "        head_output[dim_head*head_idx:dim_head*(head_idx+1)] = concated_heads_wihtout_projection[dim_head*head_idx:dim_head*(head_idx+1)]\n",
    "        head_output_projected = pre_project_v(head_output)\n",
    "        head_output_projected_meaning = model_aux.hs_to_token_top_k(head_output_projected, k_top=1, k_bottom=0)\n",
    "\n",
    "        labels.append(head_output_projected_meaning['top_k'][0])\n",
    "        curr_metadata, curr_color = get_node_customdata(head_output_projected, prefix=f'head {head_idx}: output', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "        customdata.append(curr_metadata)\n",
    "        colors_nodes.append(curr_color)\n",
    "        idx_head_output_projected = len(labels) - 1\n",
    "\n",
    "        # create a link between the head output and the concated heads\n",
    "        sources.append(idx_head_output_projected)\n",
    "        targets.append(idx_concated_heads)\n",
    "        curr_weight = get_norm_layer(head_output_projected)\n",
    "        weights.append(curr_weight*factor_head_output)\n",
    "        colors_links.append(get_color_according_to_entropy(head_output_projected))\n",
    "        line_explained.append(f'norm after projection:{curr_weight}')\n",
    "        \n",
    "        # the nodes representing the #n_values_per_head top biggest ki and vi (2)\n",
    "        best_head_vals = attentions.topk(n_values_per_head, dim=0)\n",
    "        for attn_val_idx, attn_score in zip(best_head_vals.indices, best_head_vals.values):\n",
    "            attn_score = round(attn_score.item(), round_digits)\n",
    "            keys_from_attn = keys[attn_val_idx]  # should be in the size of the subhead (for example, 64 for gpt2-medium)\n",
    "            values_from_attn = values[attn_val_idx]  # should be in the size of the subhead\n",
    "\n",
    "            keys_from_attn_proj = torch.zeros(model.config.n_embd)\n",
    "            keys_from_attn_proj[dim_head*head_idx:dim_head*(head_idx+1)] = keys_from_attn\n",
    "            keys_from_attn_proj = pre_project_k(keys_from_attn_proj)\n",
    "\n",
    "            values_from_attn_proj = torch.zeros(model.config.n_embd)\n",
    "            values_from_attn_proj[dim_head*head_idx:dim_head*(head_idx+1)] = values_from_attn\n",
    "            values_from_attn_proj = pre_project_v(values_from_attn_proj)\n",
    "\n",
    "            keys_from_attn_proj_meaning = model_aux.hs_to_token_top_k(keys_from_attn_proj, k_top=1, k_bottom=0)\n",
    "            values_from_attn_proj_meaning = model_aux.hs_to_token_top_k(values_from_attn_proj, k_top=1, k_bottom=0)\n",
    "\n",
    "            # create node for the top keys ki\n",
    "            labels.append(keys_from_attn_proj_meaning['top_k'][0])\n",
    "            if parsed_line is not None:\n",
    "                curr_metadata, curr_color = get_node_customdata(keys_from_attn_proj, prefix=f'head {head_idx}: key {attn_val_idx} [created from \"{parsed_line[attn_val_idx]}\"]', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "            else:\n",
    "                curr_metadata, curr_color = get_node_customdata(keys_from_attn_proj, prefix=f'head {head_idx}: key {attn_val_idx}', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "            customdata.append(curr_metadata)\n",
    "            colors_nodes.append(curr_color)\n",
    "            idx_keys_from_attn_proj = len(labels) - 1\n",
    "\n",
    "            # create node for the top values vi\n",
    "            labels.append(values_from_attn_proj_meaning['top_k'][0])\n",
    "            if parsed_line is not None:\n",
    "                curr_metadata, curr_color = get_node_customdata(values_from_attn_proj, prefix=f'head {head_idx}: value {attn_val_idx} [created from \"{parsed_line[attn_val_idx]}\"]', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "            else:\n",
    "                curr_metadata, curr_color = get_node_customdata(values_from_attn_proj, prefix=f'head {head_idx}: value {attn_val_idx}', top_or_bottom='top_k', layer_idx=layer_idx, target_word=target_word)\n",
    "            customdata.append(curr_metadata)\n",
    "            colors_nodes.append(curr_color)\n",
    "            idx_values_from_attn_proj = len(labels) - 1\n",
    "\n",
    "            # create a link between the query qi and the top keys ki\n",
    "            color_attn = f'rgba{cmap_attn_score(attn_score)}'\n",
    "\n",
    "            sources.append(idx_q_i_projected)\n",
    "            targets.append(idx_keys_from_attn_proj)\n",
    "            curr_weight = get_norm_layer(q_i_projected)\n",
    "            weights.append(max(attn_score*factor_attn_score, 0.51))  # in case the attentnion score is to low and the line might not be visible\n",
    "            colors_links.append(color_attn)\n",
    "            line_explained.append(f'attention score: {attn_score} (qi norm: {curr_weight})')\n",
    "\n",
    "            # create the link between the top keys and the top values\n",
    "            sources.append(idx_keys_from_attn_proj)\n",
    "            targets.append(idx_values_from_attn_proj)\n",
    "            curr_weight = get_norm_layer(keys_from_attn_proj)\n",
    "            weights.append(max(attn_score*factor_attn_score, 0.51))\n",
    "            colors_links.append(color_attn)\n",
    "            line_explained.append(f'attention score: {attn_score} (ki norm: {curr_weight})')\n",
    "\n",
    "            # create a link between the top values and the idx_head_output_projected\n",
    "            sources.append(idx_values_from_attn_proj)\n",
    "            targets.append(idx_head_output_projected)\n",
    "            curr_weight = get_norm_layer(values_from_attn_proj)\n",
    "            weights.append(max(attn_score*curr_weight*factor_for_memory_to_head_values, 0.51))\n",
    "            colors_links.append(get_color_according_to_entropy(values_from_attn_proj))\n",
    "            line_explained.append(f'attention score * norm: {attn_score*curr_weight} (vi norm: {curr_weight})')\n",
    "    \n",
    "    # now we want to present single neurons from the concatenated heads and how they are projected (indevideually) to the output by W_O (the output projection matrix)\n",
    "    # we pick only the top most activated neurons (positive and negative)\n",
    "    concated_heads_wihtout_projection_mul_norm = concated_heads_wihtout_projection * c_proj.norm(dim=1)\n",
    "    for case, n_top, is_largest in [('top_k', number_of_top_neurons, True), ('bottom_k', number_of_bottom_neurons, False)]:\n",
    "        tops = torch.topk(concated_heads_wihtout_projection_mul_norm, k=n_top, largest=is_largest)\n",
    "        for entry_idx, activision_mul_norm in zip(tops.indices, tops.values):\n",
    "            activision_mul_norm = round(activision_mul_norm.item(), round_digits)\n",
    "            entry_idx = entry_idx.item()\n",
    "            activision_value = round(concated_heads_wihtout_projection[entry_idx].item(), round_digits)\n",
    "                \n",
    "            # connect between c_proj_input, which is the concatenated heads, and each of this neurons\n",
    "            idx_value = len(labels)\n",
    "            sources.append(idx_concated_heads)\n",
    "            targets.append(idx_value)\n",
    "            curr_weight = defualt_weight\n",
    "            weights.append(curr_weight)\n",
    "            colors_links.append(positive_activation if is_largest > 0 else negative_activation)\n",
    "            curr_c_proj_meaning = model_aux.hs_to_token_top_k(c_proj[entry_idx], k_top=1, k_bottom=1)\n",
    "            labels.append(curr_c_proj_meaning[case][0])\n",
    "            curr_metadata, curr_color = get_node_customdata(c_proj[entry_idx], prefix=f'value index:{entry_idx} (from head {entry_idx//(model.config.n_embd//model.config.n_head)}), activision:{activision_value}: ', \n",
    "                            top_or_bottom=case, layer_idx=layer_idx, target_word=target_word)  # value is chosen accodring to activation sign. suppouse to reflect the meaning its adding to the output\n",
    "            customdata.append(curr_metadata)\n",
    "            colors_nodes.append(curr_color)\n",
    "            line_explained.append(f'activision:{activision_value}')\n",
    "\n",
    "            # add a link between of the last \"targets\" to the attn_output\n",
    "            sources.append(idx_value)\n",
    "            targets.append(idx_attn_out)\n",
    "            weights.append(abs(activision_mul_norm))\n",
    "            colors_links.append(get_color_according_to_entropy(c_proj[entry_idx]))\n",
    "            line_explained.append(f'activision*norm:{activision_mul_norm}')\n",
    "\n",
    "    # NOTE: in gpt2 we added the projection matrix bias (attn.c_proj.bias), however, gpt-j attention matrices\n",
    "    # don't have bias, so we skip this part of the code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_basic_graph_gpt_j(layers, hs_collector, model, model_aux=model_aux, target_word=None, line=None, save_html=False):\n",
    "    '''\n",
    "    A wrapper function to generate a graph for a given layers\n",
    "    The main different of this function from its gpt2 implementation is that it uses the function pre_connect_mlp_and_attn_for_gptj\n",
    "    before calling the creation of the attention and mlp subgraphs\n",
    "\n",
    "    @layers: a list of layers to generate the graph for (correctness is guaranteed only if layers are in order)\n",
    "    @ hs_collector: the hs_collector dictionary (created from wrapping the model with the hs_collector class)\n",
    "    @ model: the model (for example: gpt2)\n",
    "    @ model_aux: the model_aux class (more functions for the original model)\n",
    "    @ target_word: the target word for extracting the status of the neurons (ranking and probability)\n",
    "    @ line: the line that was used to generate the data in hs_collector (the prompt to the model)\n",
    "    @ save_html: if True, the graph will be saved as an html file to {title}.html. if @ save_html is a non empty string, the graph will be saved as an html file to {save_html}.html\n",
    "    '''\n",
    "\n",
    "    model = model.cpu() # all the calculations are done on the cpu (also assuming that hs_collector is on the cpu)\n",
    "\n",
    "    # init the graph data\n",
    "    graph_data = {\n",
    "        'sources': [],\n",
    "        'targets': [],\n",
    "        'weights': [],\n",
    "        'colors_nodes': [],\n",
    "        'colors_links': [],\n",
    "        'labels': [],\n",
    "        'line_explained': [],\n",
    "        'customdata': []\n",
    "    }\n",
    "\n",
    "    if type(layers) != list:\n",
    "        layers = [layers]\n",
    "    \n",
    "    # generate the graph for each layer\n",
    "    # each layer is a block of two sub-blocks: the attention block and the mlp block\n",
    "    for layer_idx in layers:\n",
    "        pre_connect_mlp_and_attn_for_gptj(layer_idx, graph_data, hs_collector=hs_collector, model=model, model_aux=model_aux, target_word=target_word)\n",
    "        layer_attn_to_graph(layer_idx, graph_data, hs_collector=hs_collector, model=model, model_aux=model_aux, target_word=target_word, line=line)\n",
    "        layer_mlp_to_graph(layer_idx, graph_data, hs_collector=hs_collector, model=model, model_aux=model_aux, target_word=target_word)\n",
    "    \n",
    "    plot_graph_aux(graph_data, title=f'Flow-Grpah of layers {layers}--> propt: \"{line}\". target: \"{target_word}\"', save_html=save_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of usage\n",
    "gen_basic_graph_gpt_j([layer_index for layer_index in range(model_config.n_layer-3, model_config.n_layer)], model=model, hs_collector=hs_collector, target_word=target_token, line=line, save_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # more examples\n",
    "gen_basic_graph_gpt_j([0], model=model, hs_collector=hs_collector, target_word=target_token, line=line, save_html=True)\n",
    "gen_basic_graph_gpt_j(8, model=model, hs_collector=hs_collector, target_word=target_token, line=line, save_html=True)\n",
    "# gen_basic_graph_gpt_j([9, 10], model=model, hs_collector=hs_collector, target_word=target_token, line=line, save_html=True)\n",
    "# gen_basic_graph_gpt_j([11], model=model, hs_collector=hs_collector, target_word=target_token, line=line, save_html=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "march222",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2601361285e90ad390a600e1cfad8ca2ead93725e8d86fc5fa8afa387269ad01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
